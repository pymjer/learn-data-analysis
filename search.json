[
  {
    "objectID": "pytorch_demo.html",
    "href": "pytorch_demo.html",
    "title": "PyTorch",
    "section": "",
    "text": "import numpy as np\nfrom torch import tensor"
  },
  {
    "objectID": "pytorch_demo.html#维度操作",
    "href": "pytorch_demo.html#维度操作",
    "title": "PyTorch",
    "section": "维度操作",
    "text": "维度操作\nYou can index with the special value [None] or use unsqueeze() to convert a 1-dimensional array into a 2-dimensional array (although one of those dimensions has value 1).\n\nnums[:,None][:3], nums.unsqueeze(1)[:3]\n\n(tensor([[0],\n         [1],\n         [2]]),\n tensor([[0],\n         [1],\n         [2]]))\n\n\n\nview修改维度\n如果需要限定维度，需要使用view方法\n\nnums_2d = nums.view(4, 3) \nnums_2d\n\ntensor([[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8],\n        [ 9, 10, 11]])\n\n\n可以省略一个维度的数量，设置为-1\n\nnums.view(4,-1)\n\ntensor([[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8],\n        [ 9, 10, 11]])\n\n\n\nnums.view(-1, 2,3)\n\ntensor([[[ 0,  1,  2],\n         [ 3,  4,  5]],\n\n        [[ 6,  7,  8],\n         [ 9, 10, 11]]])\n\n\n\nnums_2d.shape\n\ntorch.Size([4, 3])\n\n\n\n\nrepeat复制数据\nPyTorch的repeat方法主要用于副本复制张量, repeat方法会沿着指定维度重复整个输入张量,生成新的输出张量。\n\nnums = [i for i in range(12)]\nnums = tensor(nums).view(-1, 2)\nnums\n\ntensor([[ 0,  1],\n        [ 2,  3],\n        [ 4,  5],\n        [ 6,  7],\n        [ 8,  9],\n        [10, 11]])\n\n\n\nnums_rp = nums.repeat(1, 3)\nnums_rp\n\ntensor([[ 0,  1,  0,  1,  0,  1],\n        [ 2,  3,  2,  3,  2,  3],\n        [ 4,  5,  4,  5,  4,  5],\n        [ 6,  7,  6,  7,  6,  7],\n        [ 8,  9,  8,  9,  8,  9],\n        [10, 11, 10, 11, 10, 11]])\n\n\n\nnums_rp.view(-1, 2)[:3]\n\ntensor([[0, 1],\n        [0, 1],\n        [0, 1]])"
  },
  {
    "objectID": "pytorch_demo.html#expand_as-扩展维度",
    "href": "pytorch_demo.html#expand_as-扩展维度",
    "title": "PyTorch",
    "section": "expand_as 扩展维度",
    "text": "expand_as 扩展维度\n\nc = tensor([10., 20,30]); c\n\ntensor([10., 20., 30.])\n\n\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nc.expand_as(m)\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Basic",
    "section": "",
    "text": "pathlib模块提供了处理文件路径和目录路径的功能，它是Python标准库中的一部分。pathlib模块以面向对象的方式提供了简洁而直观的API，用于操作文件系统中的路径。\n\nfrom pathlib import Path\n\n# 创建路径对象\npath = Path('./test/hello')\n\n# 获取路径的各个部分\nprint('文件名:', path.name)          # file.txt\nprint('父目录:', path.parent)        # /path/to\nprint('后缀:', path.suffix)          # .txt\nprint('文件名（不含后缀）:', path.stem)  # file\n\n# 判断路径是否存在\nprint('路径是否存在:', path.exists())     # False\n\n# 创建目录\npath.parent.mkdir(parents=True, exist_ok=True)\n\n# 创建文件\npath.touch()\n\n# 写入文件内容\npath.write_text('Hello, World!')\n\n# 读取文件内容\ncontent = path.read_text()\nprint('文件内容:', content)   # Hello, World!\n\n# 遍历目录中的文件\ndirectory = Path('./test')\nfor file in directory.iterdir():\n    if file.is_file():\n        print('文件:', file.name)\n\n# 连接路径\nnew_path = path.parent / 'new_file.txt'\nprint('新路径:', new_path)    # /path/to/new_file.txt\n\n# 解析路径\nparsed_path = Path('./test/')\nprint('目录名:', parsed_path.parent.name)   # to\n\n# 规范化路径\nnormalized_path = Path('./test/')\nprint('规范化路径:', normalized_path.resolve())   # /path/file.txt\n\n# 删除文件\npath.unlink()\n\n# 验证文件是否存在\nprint('文件是否存在:', path.exists())  # False\n\n# 删除目录\ndirectory.rmdir()\n\n文件名: hello\n父目录: .\n后缀: \n文件名（不含后缀）: hello\n路径是否存在: False\n文件内容: Hello, World!\n新路径: new_file.txt\n目录名: \n规范化路径: /Users/hawkins/Work/learn/learn-data-analysis/nbs/test\n文件是否存在: False\n\n\n\n\n\nNumpy教程 User Guide\nNumPy 提供了一个 N维的数组类型， ndarray, 用于描述相同类型的items。\n\n\n\nimport numpy as np\n\nx = np.array([[1, 2, 3], [4, 5, 7]], np.int32)\ntype(x)\n\nnumpy.ndarray\n\n\n\n\n\nnp.maximum() 是 NumPy 库中的一个函数，用于比较两个数组或标量，并返回元素级别上的较大值。\n下面是 np.maximum() 的使用示例：\n\nimport numpy as np\n\n# 使用 np.maximum() 比较两个标量\nmax_scalar = np.maximum(3, 5)\nprint(max_scalar)  # 输出：5\n\n# 使用 np.maximum() 比较两个数组\narray1 = np.array([1, 2, 3, 4, 5])\narray2 = np.array([2, 4, 1, 5, 3])\nmax_array = np.maximum(array1, array2)\nprint(max_array)  # 输出：[2 4 3 5 5]\n\n5\n[2 4 3 5 5]\n\n\n在上述示例中，我们首先使用 np.maximum() 比较了两个标量，即 3 和 5。函数返回两个标量中的较大值 5。\n接下来，我们使用 np.maximum() 比较了两个数组 array1 和 array2。函数对应位置上的元素进行比较，并返回一个新的数组，其中包含每个位置上的较大值。在这个例子中，max_array 的第一个元素是 2（1 和 2 中的较大值），第二个元素是 4（2 和 4 中的较大值），以此类推。\n总结一下，np.maximum() 函数用于比较两个数组或标量，并返回一个新的数组，其中包含元素级别上的较大值。这个函数在处理数据比较和选择最大值等场景中非常有用。\n\n\n\n\nnumpy.datetime64和Python标准库中的datetime模块都是用于处理日期和时间的工具，但它们有一些不同之处。\n\n数据类型：numpy.datetime64是NumPy中专门用于处理日期和时间的数据类型，而datetime是Python标准库中的一个模块，提供了多种日期和时间处理的工具类和函数，包括日期、时间、时间差、时区等等。\n精度：numpy.datetime64的精度比datetime高，可以处理更小的时间间隔（如纳秒、微秒级别的时间间隔）。而datetime的精度最小只能到微秒级别。\n内存占用：numpy.datetime64的内存占用比datetime更小，因为它使用了固定长度的二进制格式来表示日期和时间，而datetime使用了Python的对象模型，需要更多的内存。\n运算：numpy.datetime64可以像数字一样进行运算，例如可以进行加减乘除、比较大小等操作，而datetime需要使用特定的方法来进行日期和时间的运算。\n兼容性：numpy.datetime64与NumPy的其他数据类型（如数组）可以无缝集成使用，而datetime需要进行转换后才能和其他数据类型一起使用。\n\nnumpy.datetime64和datetime都提供了一些常用的方法来处理日期和时间，例如获取当前时间、格式化输出、计算时间差等。以下是它们的一些常用方法：\n\nnumpy.datetime64的常用方法：\n\nnumpy.datetime64('now')：获取当前时间戳。\nnumpy.datetime64('2000-01-01')：创建一个指定日期的时间戳。\nnumpy.datetime64('2000-01-01') + np.timedelta64(1, 'D')：在时间戳上加上一段时间间隔。\nnumpy.datetime64('2000-01-01').astype('datetime64[M]')：将时间戳转换为指定的时间差类型。\n\ndatetime的常用方法：\n\ndatetime.datetime.now()：获取当前时间。\ndatetime.datetime(2000, 1, 1)：创建一个指定日期的时间对象。\ndatetime.datetime(2000, 1, 1) + datetime.timedelta(days=1)：在时间对象上加上一段时间间隔。\ndatetime.datetime.strftime('%Y-%m-%d')：将时间对象格式化输出为字符串。\n\nTimestamp 的常用方法：\n\npd.Timestamp(‘now’)：获取当前时间戳。\npd.Timestamp(‘2000-01-01’)：创建一个指定日期的时间戳。\npd.Timestamp(‘2000-01-01’) + pd.Timedelta(days=1)：在时间戳上加上一段时间间隔。\npd.Timestamp(‘2000-01-01’).strftime(‘%Y-%m-%d’)：将时间戳格式化输出为字符串。\n\n\n需要注意的是，numpy.datetime64和datetime的方法命名和用法有些不同，需要根据具体的需求选择合适的工具。在使用这些方法时，需要注意数据类型的转换和精度的控制，以保证计算结果的正确性和可靠性。\n\nimport numpy as np\n\nyear = 2021\nmonth = 5  \nday = 25\n\ndate = np.datetime64(f'{year}-{month:02}-{day:02}')\n\nprint(date)\n# 2021-05-25\n\n2021-05-25\n\n\n用一个整数和一个日期单位来创建datetime，时间的开始是 UNIX的开端，也就是1970年\n\nnp.datetime64(1, 'Y')\n\nnumpy.datetime64('1971')\n\n\n\nnp.array(['2007-07-13', '2008-08-09', '2009-09-12'], dtype='datetime64')\n\narray(['2007-07-13', '2008-08-09', '2009-09-12'], dtype='datetime64[D]')\n\n\n使用timedelta64进行时间计算\n\nnp.datetime64('2020-01-02')\n\nnumpy.datetime64('2020-01-22')\n\n\n\nnp.timedelta64(60, 'Y')\n\nnumpy.timedelta64(60,'Y')\n\n\nPandas中支持以下单位的时间差值：\n\n‘D’：天\n‘h’：小时\n‘m’：分钟\n‘s’：秒\n‘ms’：毫秒\n‘us’：微秒\n‘ns’：纳秒\n\n\nnp.datetime64('2009-01-05') + np.timedelta64(20, 'Y').astype('m8[D]')\n\nnumpy.datetime64('2029-01-04')\n\n\n\n# 过60年, m8[D] 和 timedelta64[D] 意思相同\nnp.datetime64('1989-04-10') + np.timedelta64(60, 'Y').astype('timedelta64[D]')\n\nnumpy.datetime64('2049-04-09')\n\n\n\n# 计算年龄, &gt; 表示向上舍入， &lt;  表示向下舍入, 当时间差的小数部分小于0.5时，向下舍入和向上舍入的结果是一致的。但是，当时间差的小数部分大于等于0.5时，向下舍入和向上舍入的结果是不同的\n(np.datetime64('now') - np.datetime64('1999-12-15')).astype('&gt;m8[Y]')\n\nnumpy.timedelta64(23,'Y')\n\n\n\nimport pandas as pd\n\ndt1 = pd.to_datetime('2020-01')\nprint(type(dt1))\n# 过去60年\ndt1_pass_60_year = dt1 + pd.Timedelta('365D') * 60\nprint(dt1_pass_60_year)\n\n&lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt;\n2079-12-17 00:00:00\n\n\n\n\n\njoblib 是一个用于高效处理 Python 对象持久化和并行计算的库。joblib 提供了 Parallel 和 delayed 对象，用于简化并行计算任务的编写和执行。\nParallel 对象用于并行执行多个函数调用，而 delayed 对象用于将函数调用延迟执行。\n下面是一个示例，展示如何使用 Parallel 和 delayed 对象进行并行计算：\nfrom joblib import Parallel, delayed\n\ndef square(x):\n    return x**2\n\ndata = [1,2,3,4,5]\n\nresults = Parallel(-1)(delayed(square)(x) for x in data)\n\nprint(results)"
  },
  {
    "objectID": "core.html#pathlib",
    "href": "core.html#pathlib",
    "title": "Basic",
    "section": "",
    "text": "pathlib模块提供了处理文件路径和目录路径的功能，它是Python标准库中的一部分。pathlib模块以面向对象的方式提供了简洁而直观的API，用于操作文件系统中的路径。\n\nfrom pathlib import Path\n\n# 创建路径对象\npath = Path('./test/hello')\n\n# 获取路径的各个部分\nprint('文件名:', path.name)          # file.txt\nprint('父目录:', path.parent)        # /path/to\nprint('后缀:', path.suffix)          # .txt\nprint('文件名（不含后缀）:', path.stem)  # file\n\n# 判断路径是否存在\nprint('路径是否存在:', path.exists())     # False\n\n# 创建目录\npath.parent.mkdir(parents=True, exist_ok=True)\n\n# 创建文件\npath.touch()\n\n# 写入文件内容\npath.write_text('Hello, World!')\n\n# 读取文件内容\ncontent = path.read_text()\nprint('文件内容:', content)   # Hello, World!\n\n# 遍历目录中的文件\ndirectory = Path('./test')\nfor file in directory.iterdir():\n    if file.is_file():\n        print('文件:', file.name)\n\n# 连接路径\nnew_path = path.parent / 'new_file.txt'\nprint('新路径:', new_path)    # /path/to/new_file.txt\n\n# 解析路径\nparsed_path = Path('./test/')\nprint('目录名:', parsed_path.parent.name)   # to\n\n# 规范化路径\nnormalized_path = Path('./test/')\nprint('规范化路径:', normalized_path.resolve())   # /path/file.txt\n\n# 删除文件\npath.unlink()\n\n# 验证文件是否存在\nprint('文件是否存在:', path.exists())  # False\n\n# 删除目录\ndirectory.rmdir()\n\n文件名: hello\n父目录: .\n后缀: \n文件名（不含后缀）: hello\n路径是否存在: False\n文件内容: Hello, World!\n新路径: new_file.txt\n目录名: \n规范化路径: /Users/hawkins/Work/learn/learn-data-analysis/nbs/test\n文件是否存在: False"
  },
  {
    "objectID": "core.html#numpy",
    "href": "core.html#numpy",
    "title": "Basic",
    "section": "",
    "text": "Numpy教程 User Guide\nNumPy 提供了一个 N维的数组类型， ndarray, 用于描述相同类型的items。\n\n\n\nimport numpy as np\n\nx = np.array([[1, 2, 3], [4, 5, 7]], np.int32)\ntype(x)\n\nnumpy.ndarray\n\n\n\n\n\nnp.maximum() 是 NumPy 库中的一个函数，用于比较两个数组或标量，并返回元素级别上的较大值。\n下面是 np.maximum() 的使用示例：\n\nimport numpy as np\n\n# 使用 np.maximum() 比较两个标量\nmax_scalar = np.maximum(3, 5)\nprint(max_scalar)  # 输出：5\n\n# 使用 np.maximum() 比较两个数组\narray1 = np.array([1, 2, 3, 4, 5])\narray2 = np.array([2, 4, 1, 5, 3])\nmax_array = np.maximum(array1, array2)\nprint(max_array)  # 输出：[2 4 3 5 5]\n\n5\n[2 4 3 5 5]\n\n\n在上述示例中，我们首先使用 np.maximum() 比较了两个标量，即 3 和 5。函数返回两个标量中的较大值 5。\n接下来，我们使用 np.maximum() 比较了两个数组 array1 和 array2。函数对应位置上的元素进行比较，并返回一个新的数组，其中包含每个位置上的较大值。在这个例子中，max_array 的第一个元素是 2（1 和 2 中的较大值），第二个元素是 4（2 和 4 中的较大值），以此类推。\n总结一下，np.maximum() 函数用于比较两个数组或标量，并返回一个新的数组，其中包含元素级别上的较大值。这个函数在处理数据比较和选择最大值等场景中非常有用。"
  },
  {
    "objectID": "core.html#datetimes-and-timedeltas",
    "href": "core.html#datetimes-and-timedeltas",
    "title": "Basic",
    "section": "",
    "text": "numpy.datetime64和Python标准库中的datetime模块都是用于处理日期和时间的工具，但它们有一些不同之处。\n\n数据类型：numpy.datetime64是NumPy中专门用于处理日期和时间的数据类型，而datetime是Python标准库中的一个模块，提供了多种日期和时间处理的工具类和函数，包括日期、时间、时间差、时区等等。\n精度：numpy.datetime64的精度比datetime高，可以处理更小的时间间隔（如纳秒、微秒级别的时间间隔）。而datetime的精度最小只能到微秒级别。\n内存占用：numpy.datetime64的内存占用比datetime更小，因为它使用了固定长度的二进制格式来表示日期和时间，而datetime使用了Python的对象模型，需要更多的内存。\n运算：numpy.datetime64可以像数字一样进行运算，例如可以进行加减乘除、比较大小等操作，而datetime需要使用特定的方法来进行日期和时间的运算。\n兼容性：numpy.datetime64与NumPy的其他数据类型（如数组）可以无缝集成使用，而datetime需要进行转换后才能和其他数据类型一起使用。\n\nnumpy.datetime64和datetime都提供了一些常用的方法来处理日期和时间，例如获取当前时间、格式化输出、计算时间差等。以下是它们的一些常用方法：\n\nnumpy.datetime64的常用方法：\n\nnumpy.datetime64('now')：获取当前时间戳。\nnumpy.datetime64('2000-01-01')：创建一个指定日期的时间戳。\nnumpy.datetime64('2000-01-01') + np.timedelta64(1, 'D')：在时间戳上加上一段时间间隔。\nnumpy.datetime64('2000-01-01').astype('datetime64[M]')：将时间戳转换为指定的时间差类型。\n\ndatetime的常用方法：\n\ndatetime.datetime.now()：获取当前时间。\ndatetime.datetime(2000, 1, 1)：创建一个指定日期的时间对象。\ndatetime.datetime(2000, 1, 1) + datetime.timedelta(days=1)：在时间对象上加上一段时间间隔。\ndatetime.datetime.strftime('%Y-%m-%d')：将时间对象格式化输出为字符串。\n\nTimestamp 的常用方法：\n\npd.Timestamp(‘now’)：获取当前时间戳。\npd.Timestamp(‘2000-01-01’)：创建一个指定日期的时间戳。\npd.Timestamp(‘2000-01-01’) + pd.Timedelta(days=1)：在时间戳上加上一段时间间隔。\npd.Timestamp(‘2000-01-01’).strftime(‘%Y-%m-%d’)：将时间戳格式化输出为字符串。\n\n\n需要注意的是，numpy.datetime64和datetime的方法命名和用法有些不同，需要根据具体的需求选择合适的工具。在使用这些方法时，需要注意数据类型的转换和精度的控制，以保证计算结果的正确性和可靠性。\n\nimport numpy as np\n\nyear = 2021\nmonth = 5  \nday = 25\n\ndate = np.datetime64(f'{year}-{month:02}-{day:02}')\n\nprint(date)\n# 2021-05-25\n\n2021-05-25\n\n\n用一个整数和一个日期单位来创建datetime，时间的开始是 UNIX的开端，也就是1970年\n\nnp.datetime64(1, 'Y')\n\nnumpy.datetime64('1971')\n\n\n\nnp.array(['2007-07-13', '2008-08-09', '2009-09-12'], dtype='datetime64')\n\narray(['2007-07-13', '2008-08-09', '2009-09-12'], dtype='datetime64[D]')\n\n\n使用timedelta64进行时间计算\n\nnp.datetime64('2020-01-02')\n\nnumpy.datetime64('2020-01-22')\n\n\n\nnp.timedelta64(60, 'Y')\n\nnumpy.timedelta64(60,'Y')\n\n\nPandas中支持以下单位的时间差值：\n\n‘D’：天\n‘h’：小时\n‘m’：分钟\n‘s’：秒\n‘ms’：毫秒\n‘us’：微秒\n‘ns’：纳秒\n\n\nnp.datetime64('2009-01-05') + np.timedelta64(20, 'Y').astype('m8[D]')\n\nnumpy.datetime64('2029-01-04')\n\n\n\n# 过60年, m8[D] 和 timedelta64[D] 意思相同\nnp.datetime64('1989-04-10') + np.timedelta64(60, 'Y').astype('timedelta64[D]')\n\nnumpy.datetime64('2049-04-09')\n\n\n\n# 计算年龄, &gt; 表示向上舍入， &lt;  表示向下舍入, 当时间差的小数部分小于0.5时，向下舍入和向上舍入的结果是一致的。但是，当时间差的小数部分大于等于0.5时，向下舍入和向上舍入的结果是不同的\n(np.datetime64('now') - np.datetime64('1999-12-15')).astype('&gt;m8[Y]')\n\nnumpy.timedelta64(23,'Y')\n\n\n\nimport pandas as pd\n\ndt1 = pd.to_datetime('2020-01')\nprint(type(dt1))\n# 过去60年\ndt1_pass_60_year = dt1 + pd.Timedelta('365D') * 60\nprint(dt1_pass_60_year)\n\n&lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt;\n2079-12-17 00:00:00"
  },
  {
    "objectID": "core.html#joblib",
    "href": "core.html#joblib",
    "title": "Basic",
    "section": "",
    "text": "joblib 是一个用于高效处理 Python 对象持久化和并行计算的库。joblib 提供了 Parallel 和 delayed 对象，用于简化并行计算任务的编写和执行。\nParallel 对象用于并行执行多个函数调用，而 delayed 对象用于将函数调用延迟执行。\n下面是一个示例，展示如何使用 Parallel 和 delayed 对象进行并行计算：\nfrom joblib import Parallel, delayed\n\ndef square(x):\n    return x**2\n\ndata = [1,2,3,4,5]\n\nresults = Parallel(-1)(delayed(square)(x) for x in data)\n\nprint(results)"
  },
  {
    "objectID": "matplot_demo.html",
    "href": "matplot_demo.html",
    "title": "Matplot",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "matplot_demo.html#折线图",
    "href": "matplot_demo.html#折线图",
    "title": "Matplot",
    "section": "折线图",
    "text": "折线图\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas._testing as tm\n\n# 创建一个包含ABCD的测试数据\ndf = tm.makeTimeDataFrame(3)\ndf.head()\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2000-01-03\n0.280970\n0.871849\n0.165805\n0.736500\n\n\n2000-01-04\n0.341058\n-0.894699\n1.261245\n-0.612476\n\n\n2000-01-05\n1.077830\n-0.809648\n-0.578658\n2.900315\n\n\n\n\n\n\n\n\n# plt.plot(df['A'], df['B'])\n# 显示图形\n# plt.show()\n\n# df.plot(x='A', y='B')\n\nfig, axes = plt.subplots(2, 1, figsize=(20, 3), height_ratios=(0.5,0.5))\ndf['A'].plot(ax=axes[0], color='b', alpha=0.5, xlabel='xx')\naxes[0].axvline(x='2000-01-04', color='r', linestyle='--', label='onset')\naxes[0].set_xlabel('Timestamp')\naxes[0].set_ylabel('onset')\naxes[0].set_title('summary')\naxes[0].legend()\n\ndf['B'].plot(ax=axes[1], color='r', alpha=0.5)\naxes[1].axvline(x='2000-01-04', color='g', linestyle='--', label='onset')\naxes[1].set_xlabel('Timestamp')\naxes[1].set_ylabel('onset')\naxes[1].set_title('summary')\naxes[1].legend()\n\nplt.show()"
  },
  {
    "objectID": "plotly_demo.html",
    "href": "plotly_demo.html",
    "title": "Plotly",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff"
  },
  {
    "objectID": "plotly_demo.html#基本图形",
    "href": "plotly_demo.html#基本图形",
    "title": "Plotly",
    "section": "基本图形",
    "text": "基本图形\n参见: plotly文档\n\nts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\nts = ts.cumsum()\nts.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf = pd.DataFrame(np.random.randn(1000,4), index=ts.index, columns=list(\"ABCD\"))\ndf = df.cumsum()\nplt.figure()\ndf.plot()\nplt.legend(loc='best')\n\n&lt;matplotlib.legend.Legend&gt;\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;"
  },
  {
    "objectID": "plotly_demo.html#折线图",
    "href": "plotly_demo.html#折线图",
    "title": "Plotly",
    "section": "折线图",
    "text": "折线图\n\ndata = np.arange(10)\ndata\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nplt.plot(data)\n\n\n\n\n\nd2 = [2,4,5,7]\nplt.plot(d2)\n\n\n\n\n\nfig = plt.figure()\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\n\n\n\n\n\n# fig.savefig('aa.jpg') # 保存图像\n\n\nnp.random.randn(50).cumsum()\n\narray([-0.65497875, -0.87539609, -1.95037822, -2.55182361, -0.95400323,\n        0.42055803, -0.02399923,  0.77984132,  0.94048919, -0.2804123 ,\n       -1.26969177, -0.8656517 , -0.73684343, -0.429114  , -2.28301825,\n       -1.20596026, -1.89069725, -0.66237253, -2.00394807, -2.07219641,\n       -3.07694994, -2.13044571, -2.27133694, -2.35227459, -2.07804544,\n       -2.65075765, -4.16473185, -5.79079532, -5.80918646, -6.886709  ,\n       -6.48238632, -6.79420837, -5.93776663, -6.65007544, -6.63828068,\n       -6.36765117, -5.36893642, -5.1920758 , -5.13324687, -4.06667904,\n       -3.46112112, -2.23527842, -2.44551486,  0.21841808,  1.82434059,\n        1.50840004,  2.83121464,  2.01716621,  1.7440124 ,  2.70610864])\n\n\n\nplt.plot(np.random.randn(50).cumsum(), 'k--')\n\n\n\n\n\ndn = np.random.randn(5)\nprint(dn)\nprint(dn.cumsum())\n\n[ 1.0926909   0.27452563  0.06956881  0.64107944 -0.46210662]\n[1.0926909  1.36721653 1.43678535 2.07786478 1.61575816]\n\n\n\ns = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0,100,10))\n\n\ns.plot()\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "plotly_demo.html#热力图",
    "href": "plotly_demo.html#热力图",
    "title": "Plotly",
    "section": "热力图",
    "text": "热力图\n\nimport plotly.figure_factory as ff\n\nz = [[1.1, .3, .5, .7, .9],\n     [1, .8, .6, .4, .2],\n     [.2, 0, .5, .7, .9],\n     [.9, .8, .4, .2, 0]]\n\nfig = ff.create_annotated_heatmap(z)\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n热力图自定义颜色\n\nimport plotly.figure_factory as ff\n\nz = [[1.1, .3, .5, .7, .9],\n     [1, .8, .6, .4, .2],\n     [.2, 0, .5, .7, .9],\n     [.9, .8, .4, .2, 0]]\n\n# 添加参数：colorscale\nfig = ff.create_annotated_heatmap(z,colorscale='Viridis')\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "plotly_demo.html#矩形图",
    "href": "plotly_demo.html#矩形图",
    "title": "Plotly",
    "section": "矩形图",
    "text": "矩形图\n柱形图有两种histogram和bar\n\nhostogram用来描述numerical变量，需要指定参数nbins，表示变量的长度\nbar用来描述categorical类型的变量，对变量进行分类\n\n\nimport plotly.express as px\ndf = px.data.tips()\nfig = px.histogram(df, x=\"total_bill\", nbins=5)\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\ndf\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\n\npx.bar(df, x=df.index, y=\"total_bill\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n调整axis\n\nimport plotly.express as px\nfig = px.bar(x=[\"a\",\"a\",\"b\",3], y=[1,2,3,4])\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfig.update_xaxes(type='category')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "plotly_demo.html#散列图scatter",
    "href": "plotly_demo.html#散列图scatter",
    "title": "Plotly",
    "section": "散列图(Scatter)",
    "text": "散列图(Scatter)\n\nimport plotly.express as px\ndf = px.data.iris()\nprint(df)\nfig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\",\n                 size='petal_length', hover_data=['petal_width'])\nfig.show()\n\n     sepal_length  sepal_width  petal_length  petal_width    species  \\\n0             5.1          3.5           1.4          0.2     setosa   \n1             4.9          3.0           1.4          0.2     setosa   \n2             4.7          3.2           1.3          0.2     setosa   \n3             4.6          3.1           1.5          0.2     setosa   \n4             5.0          3.6           1.4          0.2     setosa   \n..            ...          ...           ...          ...        ...   \n145           6.7          3.0           5.2          2.3  virginica   \n146           6.3          2.5           5.0          1.9  virginica   \n147           6.5          3.0           5.2          2.0  virginica   \n148           6.2          3.4           5.4          2.3  virginica   \n149           5.9          3.0           5.1          1.8  virginica   \n\n     species_id  \n0             1  \n1             1  \n2             1  \n3             1  \n4             1  \n..          ...  \n145           3  \n146           3  \n147           3  \n148           3  \n149           3  \n\n[150 rows x 6 columns]\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n散列图配置边缘分布图\n\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal_length\", y=\"sepal_width\",  marginal_x = \"box\",marginal_y = \"violin\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "pandas/part3_func_pandas.html",
    "href": "pandas/part3_func_pandas.html",
    "title": "Pandas Part3",
    "section": "",
    "text": "from nbdev.showdoc import *"
  },
  {
    "objectID": "pandas/part3_func_pandas.html#ewm-指数加权移动平均",
    "href": "pandas/part3_func_pandas.html#ewm-指数加权移动平均",
    "title": "Pandas Part3",
    "section": "ewm 指数加权移动平均",
    "text": "ewm 指数加权移动平均\nwm代表指数加权移动平均（Exponential Weighted Moving Average）。ewm方法用于计算指数加权移动平均，并在时间序列数据的分析和处理中非常有用。\newm方法的一般语法如下：\nDataFrame.ewm(com=alpha, span=window_span, min_periods=min_periods, adjust=True, ignore_na=False).mean()\n参数说明：\n\ncom：衰减因子（通常表示为alpha）。它控制着观测值的衰减速度，取值范围为(0, 1]。较小的值表示较快的衰减，较大的值表示较慢的衰减。com和span参数只能选择其中一个进行指定。\nspan：衰减因子对应的时间跨度。它表示衰减因子的衰减速度相当于多少个观测值的时间跨度。com和span参数只能选择其中一个进行指定。\nmin_periods：指定需要进行计算的最小观测值数量。\nadjust：是否对结果进行调整，以考虑初始值的影响。\nignore_na：是否忽略缺失值。\n\n\nimport pandas as pd\n\n# 创建示例数据\ndata = {'date': pd.date_range(start='2021-01-01', periods=20),\n        'value': [2, 2, 2, 2, 2, 2, 2, 18, 2, 2, 2, 2, 2, 2, 2, 2, 2, 18, 16, 20]}\ndf = pd.DataFrame(data)\ndf.head()\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2021-01-01\n2\n\n\n1\n2021-01-02\n2\n\n\n2\n2021-01-03\n2\n\n\n3\n2021-01-04\n2\n\n\n4\n2021-01-05\n2\n\n\n\n\n\n\n\n\ndf['value'].ewm(span=3).mean()[:3]\n\n0    2.0\n1    2.0\n2    2.0\nName: value, dtype: float64\n\n\n\n# 对 'value' 列进行指数平滑\ndf['smooth'] = df['value'].ewm(alpha=0.3).mean()\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nvalue\nsmooth\n\n\n\n\n0\n2021-01-01\n2\n2.0\n\n\n1\n2021-01-02\n2\n2.0\n\n\n2\n2021-01-03\n2\n2.0\n\n\n3\n2021-01-04\n2\n2.0\n\n\n4\n2021-01-05\n2\n2.0"
  },
  {
    "objectID": "pandas/index.html",
    "href": "pandas/index.html",
    "title": "Pandas Index",
    "section": "",
    "text": "Click through to any of these tutorials to get started with Pandas’s features.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nPandas Part1\n\n\nPandas 核心函数\n\n\n\n\nPandas Part2\n\n\nPandas 分组操作\n\n\n\n\nPandas Part3\n\n\nPandas 常用方法\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pandas/part2_applied_pandas.html",
    "href": "pandas/part2_applied_pandas.html",
    "title": "Pandas Part2",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas._testing as tm\nfrom pathlib import Path\n\nBASE = Path('../../input/')"
  },
  {
    "objectID": "pandas/part2_applied_pandas.html#selecting-with-a-multiindex",
    "href": "pandas/part2_applied_pandas.html#selecting-with-a-multiindex",
    "title": "Pandas Part2",
    "section": "Selecting with a MultiIndex",
    "text": "Selecting with a MultiIndex\n使用tuple来选择特定的列\n\nneighborhoods['Services']\n\n\n\n\n\n\n\n\n\nSubcategory\nPolice\nSchools\n\n\nState\nCity\nStreet\n\n\n\n\n\n\nMO1\nFisherborough3\n244 Tracy View\nA-\nA+\n\n\n245 Tracy View\nD-\nA+\n\n\n246 Tracy View\nD-\nA+\n\n\n247 Tracy View\nD-\nA+\n\n\n248 Tracy View\nD-\nA+\n\n\nMO2\nFisherborough4\n249 Tracy View\nD-\nA+\n\n\n250 Tracy View\nD-\nA+\n\n\n251 Tracy View\nD-\nA+\n\n\n252 Tracy View\nD-\nA—\n\n\nAR\nAllisonland 124\nDiaz Brooks\nF\nC+\n\n\nGA\nAmyburgh 941\nBrian Ex\nD-\nC+\n\n\nIA\nAmyburgh 163\nHeather\nA+\nA-\n\n\n\n\n\n\n\n\nneighborhoods[('Services', \"Schools\")]\n\nState  City             Street        \nMO1    Fisherborough3   244 Tracy View    A+\n                        245 Tracy View    A+\n                        246 Tracy View    A+\n                        247 Tracy View    A+\n                        248 Tracy View    A+\nMO2    Fisherborough4   249 Tracy View    A+\n                        250 Tracy View    A+\n                        251 Tracy View    A+\n                        252 Tracy View    A—\nAR     Allisonland 124  Diaz Brooks       C+\nGA     Amyburgh 941     Brian Ex          C+\nIA     Amyburgh 163     Heather           A-\nName: (Services, Schools), dtype: object\n\n\n\nneighborhoods[[('Services', \"Schools\"), (\"Culture\", \"Museums\")]]\n\n\n\n\n\n\n\n\n\nCategory\nServices\nCulture\n\n\n\n\nSubcategory\nSchools\nMuseums\n\n\nState\nCity\nStreet\n\n\n\n\n\n\nMO1\nFisherborough3\n244 Tracy View\nA+\nA\n\n\n245 Tracy View\nA+\nF\n\n\n246 Tracy View\nA+\nF\n\n\n247 Tracy View\nA+\nF\n\n\n248 Tracy View\nA+\nF\n\n\nMO2\nFisherborough4\n249 Tracy View\nA+\nF\n\n\n250 Tracy View\nA+\nF\n\n\n251 Tracy View\nA+\nF\n\n\n252 Tracy View\nA—\nF\n\n\nAR\nAllisonland 124\nDiaz Brooks\nC+\nA+\n\n\nGA\nAmyburgh 941\nBrian Ex\nC+\nB\n\n\nIA\nAmyburgh 163\nHeather\nA-\nD\n\n\n\n\n\n\n\n\nneighborhoods.loc[\"MO1\"]\n\n\n\n\n\n\n\n\nCategory\nCulture\nServices\n\n\n\nSubcategory\nRestaurants\nMuseums\nPolice\nSchools\n\n\nCity\nStreet\n\n\n\n\n\n\n\n\nFisherborough3\n244 Tracy View\nA+\nA\nA-\nA+\n\n\n245 Tracy View\nC+\nF\nD-\nA+\n\n\n246 Tracy View\nC+\nF\nD-\nA+\n\n\n247 Tracy View\nC+\nF\nD-\nA+\n\n\n248 Tracy View\nC+\nF\nD-\nA+\n\n\n\n\n\n\n\n\nneighborhoods.loc[\"MO2\", \"Fisherborough4\"]\n\n/var/folders/5s/hp2lrr6s28dcft7pjdnx9w3c0000gn/T/ipykernel_53050/1088001906.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.\n  neighborhoods.loc[\"MO2\", \"Fisherborough4\"]\n\n\n\n\n\n\n\n\nCategory\nCulture\nServices\n\n\nSubcategory\nRestaurants\nMuseums\nPolice\nSchools\n\n\nStreet\n\n\n\n\n\n\n\n\n249 Tracy View\nC+\nF\nD-\nA+\n\n\n250 Tracy View\nC+\nF\nD-\nA+\n\n\n251 Tracy View\nC+\nF\nD-\nA+\n\n\n252 Tracy View\nC+\nF\nD-\nA—\n\n\n\n\n\n\n\n\nneighborhoods.loc[(\"MO2\", \"Fisherborough4\")]\n\n/var/folders/5s/hp2lrr6s28dcft7pjdnx9w3c0000gn/T/ipykernel_53050/3316804197.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.\n  neighborhoods.loc[(\"MO2\", \"Fisherborough4\")]\n\n\n\n\n\n\n\n\nCategory\nCulture\nServices\n\n\nSubcategory\nRestaurants\nMuseums\nPolice\nSchools\n\n\nStreet\n\n\n\n\n\n\n\n\n249 Tracy View\nC+\nF\nD-\nA+\n\n\n250 Tracy View\nC+\nF\nD-\nA+\n\n\n251 Tracy View\nC+\nF\nD-\nA+\n\n\n252 Tracy View\nC+\nF\nD-\nA—\n\n\n\n\n\n\n\n\nneighborhoods.loc[(\"MO1\",\"Fisherborough3\", \"244 Tracy View\")]\n\nCategory  Subcategory\nCulture   Restaurants    A+\n          Museums         A\nServices  Police         A-\n          Schools        A+\nName: (MO1, Fisherborough3, 244 Tracy View), dtype: object\n\n\n定位到具体的列\n\nneighborhoods.iloc[0:2]\n\n\n\n\n\n\n\n\n\nCategory\nCulture\nServices\n\n\n\n\nSubcategory\nRestaurants\nMuseums\nPolice\nSchools\n\n\nState\nCity\nStreet\n\n\n\n\n\n\n\n\nMO1\nFisherborough3\n244 Tracy View\nA+\nA\nA-\nA+\n\n\n245 Tracy View\nC+\nF\nD-\nA+\n\n\n\n\n\n\n\n\nneighborhoods.loc[(\"MO1\",\"Fisherborough3\"), (\"Services\")]\n\n/var/folders/5s/hp2lrr6s28dcft7pjdnx9w3c0000gn/T/ipykernel_53050/1488621064.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.\n  neighborhoods.loc[(\"MO1\",\"Fisherborough3\"), (\"Services\")]\n\n\n\n\n\n\n\n\nSubcategory\nPolice\nSchools\n\n\nStreet\n\n\n\n\n\n\n244 Tracy View\nA-\nA+\n\n\n245 Tracy View\nD-\nA+\n\n\n246 Tracy View\nD-\nA+\n\n\n247 Tracy View\nD-\nA+\n\n\n248 Tracy View\nD-\nA+"
  },
  {
    "objectID": "pandas/part2_applied_pandas.html#creating-a-groupby-object-from-scratch",
    "href": "pandas/part2_applied_pandas.html#creating-a-groupby-object-from-scratch",
    "title": "Pandas Part2",
    "section": "9.1 Creating a GroupBy Object from Scratch",
    "text": "9.1 Creating a GroupBy Object from Scratch\n\nimport pandas as pd\n\n\nfood_data = {\n          \"Item\": [\"Banana\", \"Cucumber\", \"Orange\", \"Tomato\", \"Watermelon\"],\n          \"Type\": [\"Fruit\", \"Vegetable\", \"Fruit\", \"Vegetable\", \"Fruit\"],\n          \"Price\": [0.99, 1.25, 0.25, 0.33, 3.00]\n        }\n \nsupermarket = pd.DataFrame(data = food_data)\n\nsupermarket\n\n\n\n\n\n\n\n\nItem\nType\nPrice\n\n\n\n\n0\nBanana\nFruit\n0.99\n\n\n1\nCucumber\nVegetable\n1.25\n\n\n2\nOrange\nFruit\n0.25\n\n\n3\nTomato\nVegetable\n0.33\n\n\n4\nWatermelon\nFruit\n3.00\n\n\n\n\n\n\n\n\npd.get_dummies(supermarket.Type, prefix=\"Type\")\n\n\n\n\n\n\n\n\nType_Fruit\nType_Vegetable\n\n\n\n\n0\nTrue\nFalse\n\n\n1\nFalse\nTrue\n\n\n2\nTrue\nFalse\n\n\n3\nFalse\nTrue\n\n\n4\nTrue\nFalse\n\n\n\n\n\n\n\n\ngroups = supermarket.groupby(\"Type\") \ngroups\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object&gt;\n\n\n\ngroups.get_group(\"Fruit\")\n\n\n\n\n\n\n\n\nItem\nType\nPrice\n\n\n\n\n0\nBanana\nFruit\n0.99\n\n\n2\nOrange\nFruit\n0.25\n\n\n4\nWatermelon\nFruit\n3.00\n\n\n\n\n\n\n\n\ngroups.Price.mean()\n\nType\nFruit        1.413333\nVegetable    0.790000\nName: Price, dtype: float64\n\n\n\n# transform 会返回一个和原始DataFrame结构相同的对象，每个对象的值还是和分组计算的值相同\ngroups.Price.transform(\"mean\")\n\n0    1.413333\n1    0.790000\n2    1.413333\n3    0.790000\n4    1.413333\nName: Price, dtype: float64"
  },
  {
    "objectID": "pandas/part2_applied_pandas.html#creating-a-groupby-object-from-a-dataset",
    "href": "pandas/part2_applied_pandas.html#creating-a-groupby-object-from-a-dataset",
    "title": "Pandas Part2",
    "section": "9.2 Creating a GroupBy Object from a Dataset",
    "text": "9.2 Creating a GroupBy Object from a Dataset\n\nfortune = pd.read_csv(BASE / \"pandas-in-action/fortune1000.csv\")\nfortune\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nSector\nIndustry\n\n\n\n\n0\nWalmart\n500343.0\n9862.0\n2300000\nRetailing\nGeneral Merchandisers\n\n\n1\nExxon Mobil\n244363.0\n19710.0\n71200\nEnergy\nPetroleum Refining\n\n\n2\nBerkshire Hathaway\n242137.0\n44940.0\n377000\nFinancials\nInsurance: Property and Casualty (Stock)\n\n\n3\nApple\n229234.0\n48351.0\n123000\nTechnology\nComputers, Office Equipment\n\n\n4\nUnitedHealth Group\n201159.0\n10558.0\n260000\nHealth Care\nHealth Care: Insurance and Managed Care\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n995\nSiteOne Landscape Supply\n1862.0\n54.6\n3664\nWholesalers\nWholesalers: Diversified\n\n\n996\nCharles River Laboratories Intl\n1858.0\n123.4\n11800\nHealth Care\nHealth Care: Pharmacy and Other Services\n\n\n997\nCoreLogic\n1851.0\n152.2\n5900\nBusiness Services\nFinancial Data Services\n\n\n998\nEnsign Group\n1849.0\n40.5\n21301\nHealth Care\nHealth Care: Medical Facilities\n\n\n999\nHCP\n1848.0\n414.2\n190\nFinancials\nReal estate\n\n\n\n\n1000 rows × 6 columns\n\n\n\n\nin_retailing = fortune.Sector == \"Retailing\"\nretail_companies = fortune[in_retailing]\nretail_companies.head()\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nSector\nIndustry\n\n\n\n\n0\nWalmart\n500343.0\n9862.0\n2300000\nRetailing\nGeneral Merchandisers\n\n\n7\nAmazon.com\n177866.0\n3033.0\n566000\nRetailing\nInternet Services and Retailing\n\n\n14\nCostco\n129025.0\n2679.0\n182000\nRetailing\nGeneral Merchandisers\n\n\n22\nHome Depot\n100904.0\n8630.0\n413000\nRetailing\nSpecialty Retailers: Other\n\n\n38\nTarget\n71879.0\n2934.0\n345000\nRetailing\nGeneral Merchandisers\n\n\n\n\n\n\n\n\nretail_companies.Revenues.mean()\n\n21874.714285714286\n\n\n\nsectors = fortune.groupby(\"Sector\")\nsectors\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object&gt;\n\n\n\nlen(sectors)\n\n21\n\n\n\n# 查看每个分组的数量，按字母表排序\nsectors.size()\n\nSector\nAerospace & Defense               25\nApparel                           14\nBusiness Services                 53\nChemicals                         33\nEnergy                           107\nEngineering & Construction        27\nFinancials                       155\nFood &  Drug Stores               12\nFood, Beverages & Tobacco         37\nHealth Care                       71\nHotels, Restaurants & Leisure     26\nHousehold Products                28\nIndustrials                       49\nMaterials                         45\nMedia                             25\nMotor Vehicles & Parts            19\nRetailing                         77\nTechnology                       103\nTelecommunications                10\nTransportation                    40\nWholesalers                       44\ndtype: int64"
  },
  {
    "objectID": "pandas/part2_applied_pandas.html#attributes-and-methods-of-a-groupby-object",
    "href": "pandas/part2_applied_pandas.html#attributes-and-methods-of-a-groupby-object",
    "title": "Pandas Part2",
    "section": "9.3 Attributes and methods of a GroupBy object",
    "text": "9.3 Attributes and methods of a GroupBy object\n\n# 本质上，GroupBy对象是一个字典，存储了每个分组的索引label\nsectors.groups\n\n{'Aerospace & Defense': [26, 50, 58, 98, 117, 118, 207, 224, 275, 380, 404, 406, 414, 540, 660, 661, 806, 829, 884, 930, 954, 955, 959, 975, 988], 'Apparel': [88, 241, 331, 420, 432, 526, 529, 554, 587, 678, 766, 774, 835, 861], 'Business Services': [142, 160, 187, 199, 201, 221, 235, 242, 253, 295, 325, 358, 364, 423, 462, 465, 486, 493, 497, 499, 502, 510, 528, 567, 577, 584, 591, 599, 604, 618, 649, 686, 691, 692, 700, 702, 712, 720, 738, 744, 771, 802, 810, 825, 879, 888, 894, 895, 898, 905, 922, 972, 997], 'Chemicals': [46, 189, 190, 198, 214, 263, 281, 309, 344, 351, 381, 447, 450, 454, 527, 593, 623, 648, 671, 672, 679, 704, 722, 740, 790, 836, 865, 872, 908, 932, 958, 963, 978], 'Energy': [1, 12, 27, 30, 40, 63, 89, 90, 91, 94, 104, 114, 124, 125, 134, 145, 166, 167, 175, 184, 205, 212, 213, 217, 218, 219, 222, 231, 232, 243, 248, 254, 256, 265, 268, 269, 270, 273, 307, 313, 326, 333, 335, 343, 352, 363, 371, 379, 383, 384, 387, 428, 437, 452, 456, 488, 490, 496, 498, 500, 517, 534, 537, 559, 563, 582, 603, 617, 635, 638, 639, 647, 653, 668, 681, 682, 689, 708, 719, 723, 739, 768, 791, 798, 805, 812, 816, 818, 820, 826, 844, 859, 860, 866, 869, 885, 890, 901, 917, 934, ...], 'Engineering & Construction': [152, 163, 210, 229, 296, 315, 338, 340, 367, 427, 443, 479, 541, 569, 588, 615, 705, 736, 737, 764, 779, 815, 841, 852, 962, 982, 986], 'Financials': [2, 19, 20, 23, 25, 31, 35, 37, 42, 51, 59, 65, 66, 67, 68, 69, 78, 83, 85, 92, 99, 100, 102, 103, 105, 111, 121, 135, 136, 155, 164, 172, 174, 204, 206, 209, 211, 216, 233, 236, 238, 240, 249, 251, 252, 258, 262, 266, 298, 301, 302, 306, 310, 312, 329, 336, 342, 347, 355, 365, 368, 374, 375, 377, 397, 411, 412, 418, 430, 434, 436, 439, 440, 449, 453, 457, 459, 461, 463, 466, 468, 469, 475, 476, 482, 483, 485, 492, 501, 513, 514, 533, 539, 543, 562, 570, 572, 576, 579, 581, ...], 'Food &  Drug Stores': [16, 18, 52, 87, 93, 179, 549, 552, 600, 667, 839, 941], 'Food, Beverages & Tobacco': [44, 47, 79, 86, 95, 107, 113, 116, 153, 181, 215, 225, 274, 320, 322, 357, 361, 378, 382, 385, 417, 445, 477, 480, 511, 538, 575, 611, 628, 633, 683, 694, 717, 734, 759, 900, 937], 'Health Care': [4, 6, 24, 28, 36, 48, 55, 56, 60, 62, 72, 77, 109, 110, 115, 128, 129, 144, 146, 151, 159, 161, 169, 178, 197, 223, 239, 244, 250, 267, 282, 285, 303, 327, 360, 366, 389, 415, 472, 474, 503, 506, 518, 542, 555, 560, 602, 605, 655, 669, 673, 718, 724, 742, 753, 754, 782, 795, 830, 845, 915, 921, 925, 935, 936, 939, 956, 960, 994, 996, 998], 'Hotels, Restaurants & Leisure': [126, 130, 131, 226, 279, 323, 395, 396, 446, 471, 478, 535, 548, 556, 583, 713, 714, 721, 741, 775, 817, 851, 891, 903, 965, 984], 'Household Products': [41, 162, 183, 195, 227, 257, 314, 370, 372, 421, 467, 484, 508, 512, 532, 608, 622, 727, 778, 794, 821, 875, 881, 882, 893, 912, 946, 967], 'Industrials': [17, 64, 76, 96, 101, 139, 148, 154, 177, 203, 255, 292, 341, 346, 359, 413, 419, 444, 546, 571, 601, 616, 619, 620, 626, 627, 631, 634, 664, 687, 730, 731, 750, 776, 803, 813, 827, 850, 853, 854, 857, 864, 878, 887, 938, 943, 951, 966, 981], 'Materials': [123, 150, 193, 245, 261, 276, 304, 311, 337, 393, 398, 399, 403, 409, 426, 435, 441, 455, 460, 516, 522, 524, 564, 594, 595, 596, 606, 609, 614, 624, 641, 662, 663, 685, 695, 707, 729, 756, 781, 786, 833, 837, 945, 957, 974], 'Media': [54, 97, 108, 196, 220, 289, 349, 376, 405, 408, 451, 592, 645, 650, 654, 710, 715, 728, 735, 767, 785, 819, 847, 961, 993], 'Motor Vehicles & Parts': [9, 10, 147, 186, 259, 288, 300, 319, 391, 392, 448, 643, 699, 716, 758, 886, 889, 920, 940], 'Retailing': [0, 7, 14, 22, 38, 39, 71, 84, 119, 122, 133, 137, 138, 156, 171, 173, 180, 182, 208, 230, 234, 247, 271, 272, 277, 278, 280, 287, 293, 294, 297, 316, 321, 328, 332, 334, 339, 362, 390, 422, 424, 433, 438, 458, 464, 470, 504, 507, 509, 545, 547, 580, 589, 610, 612, 621, 644, 665, 680, 698, 732, 745, 746, 770, 777, 792, 822, 846, 855, 856, 858, 880, 909, 926, 971, 989, 992], 'Technology': [3, 21, 29, 33, 34, 45, 57, 61, 75, 81, 106, 132, 143, 149, 157, 158, 188, 191, 194, 200, 228, 260, 284, 290, 291, 305, 308, 353, 373, 388, 400, 402, 410, 416, 431, 442, 481, 491, 494, 505, 515, 519, 523, 525, 536, 544, 550, 551, 553, 557, 558, 573, 578, 585, 586, 597, 636, 646, 656, 666, 674, 696, 697, 703, 709, 726, 752, 755, 763, 769, 780, 783, 787, 788, 797, 799, 807, 823, 824, 828, 834, 842, 843, 849, 862, 868, 871, 874, 876, 892, 897, 902, 919, 924, 927, 931, 944, 948, 969, 970, ...], 'Telecommunications': [8, 15, 32, 73, 165, 202, 324, 473, 520, 907], 'Transportation': [43, 49, 70, 74, 80, 140, 141, 185, 192, 264, 283, 354, 386, 394, 401, 407, 487, 495, 530, 531, 566, 598, 632, 637, 675, 688, 706, 762, 793, 801, 814, 848, 904, 906, 914, 918, 928, 933, 947, 968], 'Wholesalers': [5, 11, 13, 53, 82, 112, 120, 127, 168, 170, 176, 237, 246, 286, 299, 317, 318, 330, 345, 348, 350, 356, 369, 425, 429, 489, 521, 561, 565, 568, 574, 642, 652, 670, 684, 748, 757, 773, 800, 809, 870, 910, 942, 995]}\n\n\n\nfortune.loc[26, \"Sector\"]\n\n'Aerospace & Defense'\n\n\n\n# 查看每个分组的第一行\nsectors.first()\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nIndustry\n\n\nSector\n\n\n\n\n\n\n\n\n\nAerospace & Defense\nBoeing\n93392.0\n8197.0\n140800\nAerospace and Defense\n\n\nApparel\nNike\n34350.0\n4240.0\n74400\nApparel\n\n\nBusiness Services\nManpowerGroup\n21034.0\n545.4\n29000\nTemporary Help\n\n\nChemicals\nDowDuPont\n62683.0\n1460.0\n98000\nChemicals\n\n\nEnergy\nExxon Mobil\n244363.0\n19710.0\n71200\nPetroleum Refining\n\n\nEngineering & Construction\nFluor\n19521.0\n191.4\n56706\nEngineering, Construction\n\n\nFinancials\nBerkshire Hathaway\n242137.0\n44940.0\n377000\nInsurance: Property and Casualty (Stock)\n\n\nFood & Drug Stores\nKroger\n122662.0\n1907.0\n449000\nFood and Drug Stores\n\n\nFood, Beverages & Tobacco\nPepsiCo\n63525.0\n4857.0\n263000\nFood Consumer Products\n\n\nHealth Care\nUnitedHealth Group\n201159.0\n10558.0\n260000\nHealth Care: Insurance and Managed Care\n\n\nHotels, Restaurants & Leisure\nMarriott International\n22894.0\n1372.0\n177000\nHotels, Casinos, Resorts\n\n\nHousehold Products\nProcter & Gamble\n66217.0\n15326.0\n95000\nHousehold and Personal Products\n\n\nIndustrials\nGeneral Electric\n122274.0\n-5786.0\n313000\nIndustrial Machinery\n\n\nMaterials\nInternational Paper\n23302.0\n2144.0\n56000\nPackaging, Containers\n\n\nMedia\nDisney\n55137.0\n8980.0\n199000\nEntertainment\n\n\nMotor Vehicles & Parts\nGeneral Motors\n157311.0\n-3864.0\n180000\nMotor Vehicles and Parts\n\n\nRetailing\nWalmart\n500343.0\n9862.0\n2300000\nGeneral Merchandisers\n\n\nTechnology\nApple\n229234.0\n48351.0\n123000\nComputers, Office Equipment\n\n\nTelecommunications\nAT&T\n160546.0\n29450.0\n254000\nTelecommunications\n\n\nTransportation\nUPS\n65872.0\n4910.0\n346415\nMail, Package, and Freight Delivery\n\n\nWholesalers\nMcKesson\n198533.0\n5070.0\n64500\nWholesalers: Health Care\n\n\n\n\n\n\n\n\nsectors.last()\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nIndustry\n\n\nSector\n\n\n\n\n\n\n\n\n\nAerospace & Defense\nAerojet Rocketdyne Holdings\n1877.0\n-9.2\n5157\nAerospace and Defense\n\n\nApparel\nWolverine World Wide\n2350.0\n0.3\n3700\nApparel\n\n\nBusiness Services\nCoreLogic\n1851.0\n152.2\n5900\nFinancial Data Services\n\n\nChemicals\nStepan\n1925.0\n91.6\n2096\nChemicals\n\n\nEnergy\nSuperior Energy Services\n1874.0\n-205.9\n6400\nOil and Gas Equipment, Services\n\n\nEngineering & Construction\nTopBuild\n1906.0\n158.1\n8400\nEngineering, Construction\n\n\nFinancials\nHCP\n1848.0\n414.2\n190\nReal estate\n\n\nFood & Drug Stores\nFreds\n2064.0\n-140.3\n7324\nFood and Drug Stores\n\n\nFood, Beverages & Tobacco\nUniversal\n2071.0\n106.3\n24000\nTobacco\n\n\nHealth Care\nEnsign Group\n1849.0\n40.5\n21301\nHealth Care: Medical Facilities\n\n\nHotels, Restaurants & Leisure\nVail Resorts\n1907.0\n210.6\n20150\nHotels, Casinos, Resorts\n\n\nHousehold Products\nACCO Brands\n1949.0\n131.7\n6620\nHome Equipment, Furnishings\n\n\nIndustrials\nRexnord\n1918.0\n74.1\n8000\nIndustrial Machinery\n\n\nMaterials\nSummit Materials\n1933.0\n121.8\n6000\nBuilding Materials, Glass\n\n\nMedia\nTribune Media\n1867.0\n194.1\n6000\nEntertainment\n\n\nMotor Vehicles & Parts\nTower International\n2066.0\n47.6\n7600\nMotor Vehicles and Parts\n\n\nRetailing\nChildrens Place\n1870.0\n84.7\n9800\nSpecialty Retailers: Apparel\n\n\nTechnology\nVeriFone Systems\n1871.0\n-173.8\n5600\nFinancial Data Services\n\n\nTelecommunications\nZayo Group Holdings\n2200.0\n85.7\n3794\nTelecommunications\n\n\nTransportation\nEcho Global Logistics\n1943.0\n12.6\n2453\nTransportation and Logistics\n\n\nWholesalers\nSiteOne Landscape Supply\n1862.0\n54.6\n3664\nWholesalers: Diversified\n\n\n\n\n\n\n\n\n# 获取任意一行的数据\nsectors.nth(3)\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nSector\nIndustry\n\n\n\n\n22\nHome Depot\n100904.0\n8630.0\n413000\nRetailing\nSpecialty Retailers: Other\n\n\n23\nBank of America Corp.\n100264.0\n18232.0\n209376\nFinancials\nCommercial Banks\n\n\n28\nAnthem\n90040.0\n3842.8\n56000\nHealth Care\nHealth Care: Insurance and Managed Care\n\n\n30\nValero Energy\n88407.0\n4065.0\n10015\nEnergy\nPetroleum Refining\n\n\n33\nIBM\n79139.0\n5753.0\n397800\nTechnology\nInformation Technology Services\n\n\n53\nSysco\n55371.0\n1142.5\n66500\nWholesalers\nWholesalers: Food and Grocery\n\n\n73\nCharter Communications\n41581.0\n9895.0\n94800\nTelecommunications\nTelecommunications\n\n\n74\nDelta Air Lines\n41244.0\n3577.0\n86564\nTransportation\nAirlines\n\n\n86\nCoca-Cola\n35410.0\n1248.0\n61800\nFood, Beverages & Tobacco\nBeverages\n\n\n87\nPublix Super Markets\n34837.0\n2291.9\n193000\nFood & Drug Stores\nFood and Drug Stores\n\n\n96\n3M\n31657.0\n4858.0\n91536\nIndustrials\nMiscellaneous\n\n\n98\nGeneral Dynamics\n30973.0\n2912.0\n98600\nAerospace & Defense\nAerospace and Defense\n\n\n186\nGoodyear Tire & Rubber\n15377.0\n346.0\n64000\nMotor Vehicles & Parts\nMotor Vehicles and Parts\n\n\n195\nNewell Brands\n14742.0\n2748.8\n49000\nHousehold Products\nHome Equipment, Furnishings\n\n\n196\nCBS\n14710.0\n357.0\n14715\nMedia\nEntertainment\n\n\n198\nMonsanto\n14640.0\n2260.0\n21900\nChemicals\nChemicals\n\n\n199\nAramark\n14604.0\n373.9\n215000\nBusiness Services\nDiversified Outsourcing Services\n\n\n226\nLas Vegas Sands\n12882.0\n2806.0\n50500\nHotels, Restaurants & Leisure\nHotels, Casinos, Resorts\n\n\n229\nLennar\n12646.0\n810.5\n9111\nEngineering & Construction\nHomebuilders\n\n\n245\nUnited States Steel\n12250.0\n387.0\n29200\nMaterials\nMetals\n\n\n420\nRalph Lauren\n6653.0\n-99.3\n18250\nApparel\nApparel\n\n\n\n\n\n\n\n\nfortune[fortune.Sector == \"Aerospace & Defense\"].iloc[3]\n\nCompany           General Dynamics\nRevenues                   30973.0\nProfits                     2912.0\nEmployees                    98600\nSector         Aerospace & Defense\nIndustry     Aerospace and Defense\nName: 98, dtype: object\n\n\n\n# 获取每个分区的前两行\nsectors.head(2)\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nSector\nIndustry\n\n\n\n\n0\nWalmart\n500343.0\n9862.0\n2300000\nRetailing\nGeneral Merchandisers\n\n\n1\nExxon Mobil\n244363.0\n19710.0\n71200\nEnergy\nPetroleum Refining\n\n\n2\nBerkshire Hathaway\n242137.0\n44940.0\n377000\nFinancials\nInsurance: Property and Casualty (Stock)\n\n\n3\nApple\n229234.0\n48351.0\n123000\nTechnology\nComputers, Office Equipment\n\n\n4\nUnitedHealth Group\n201159.0\n10558.0\n260000\nHealth Care\nHealth Care: Insurance and Managed Care\n\n\n5\nMcKesson\n198533.0\n5070.0\n64500\nWholesalers\nWholesalers: Health Care\n\n\n6\nCVS Health\n184765.0\n6622.0\n203000\nHealth Care\nHealth Care: Pharmacy and Other Services\n\n\n7\nAmazon.com\n177866.0\n3033.0\n566000\nRetailing\nInternet Services and Retailing\n\n\n8\nAT&T\n160546.0\n29450.0\n254000\nTelecommunications\nTelecommunications\n\n\n9\nGeneral Motors\n157311.0\n-3864.0\n180000\nMotor Vehicles & Parts\nMotor Vehicles and Parts\n\n\n10\nFord Motor\n156776.0\n7602.0\n202000\nMotor Vehicles & Parts\nMotor Vehicles and Parts\n\n\n11\nAmerisourceBergen\n153144.0\n364.5\n19500\nWholesalers\nWholesalers: Health Care\n\n\n12\nChevron\n134533.0\n9195.0\n51900\nEnergy\nPetroleum Refining\n\n\n15\nVerizon\n126034.0\n30101.0\n155400\nTelecommunications\nTelecommunications\n\n\n16\nKroger\n122662.0\n1907.0\n449000\nFood & Drug Stores\nFood and Drug Stores\n\n\n17\nGeneral Electric\n122274.0\n-5786.0\n313000\nIndustrials\nIndustrial Machinery\n\n\n18\nWalgreens Boots Alliance\n118214.0\n4078.0\n290000\nFood & Drug Stores\nFood and Drug Stores\n\n\n19\nJPMorgan Chase\n113899.0\n24441.0\n252539\nFinancials\nCommercial Banks\n\n\n21\nAlphabet\n110855.0\n12662.0\n80110\nTechnology\nInternet Services and Retailing\n\n\n26\nBoeing\n93392.0\n8197.0\n140800\nAerospace & Defense\nAerospace and Defense\n\n\n41\nProcter & Gamble\n66217.0\n15326.0\n95000\nHousehold Products\nHousehold and Personal Products\n\n\n43\nUPS\n65872.0\n4910.0\n346415\nTransportation\nMail, Package, and Freight Delivery\n\n\n44\nPepsiCo\n63525.0\n4857.0\n263000\nFood, Beverages & Tobacco\nFood Consumer Products\n\n\n46\nDowDuPont\n62683.0\n1460.0\n98000\nChemicals\nChemicals\n\n\n47\nArcher Daniels Midland\n60828.0\n1595.0\n31300\nFood, Beverages & Tobacco\nFood Production\n\n\n49\nFedEx\n60319.0\n2997.0\n357000\nTransportation\nMail, Package, and Freight Delivery\n\n\n50\nUnited Technologies\n59837.0\n4552.0\n204700\nAerospace & Defense\nAerospace and Defense\n\n\n54\nDisney\n55137.0\n8980.0\n199000\nMedia\nEntertainment\n\n\n64\nCaterpillar\n45462.0\n754.0\n98400\nIndustrials\nConstruction and Farm Machinery\n\n\n88\nNike\n34350.0\n4240.0\n74400\nApparel\nApparel\n\n\n97\nTime Warner\n31271.0\n5247.0\n26000\nMedia\nEntertainment\n\n\n123\nInternational Paper\n23302.0\n2144.0\n56000\nMaterials\nPackaging, Containers\n\n\n126\nMarriott International\n22894.0\n1372.0\n177000\nHotels, Restaurants & Leisure\nHotels, Casinos, Resorts\n\n\n130\nMcDonalds\n22820.0\n5192.3\n235000\nHotels, Restaurants & Leisure\nFood Services\n\n\n142\nManpowerGroup\n21034.0\n545.4\n29000\nBusiness Services\nTemporary Help\n\n\n150\nNucor\n20252.0\n1318.7\n25100\nMaterials\nMetals\n\n\n152\nFluor\n19521.0\n191.4\n56706\nEngineering & Construction\nEngineering, Construction\n\n\n160\nVisa\n18358.0\n6699.0\n15000\nBusiness Services\nFinancial Data Services\n\n\n162\nKimberly-Clark\n18259.0\n2278.0\n42000\nHousehold Products\nHousehold and Personal Products\n\n\n163\nAECOM\n18203.0\n339.4\n87000\nEngineering & Construction\nEngineering, Construction\n\n\n189\nSherwin-Williams\n14984.0\n1772.3\n52695\nChemicals\nChemicals\n\n\n241\nVF\n12400.0\n614.9\n69000\nApparel\nApparel\n\n\n\n\n\n\n\n\nsectors.tail(3)\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nSector\nIndustry\n\n\n\n\n473\nWindstream Holdings\n5853.0\n-2116.6\n12979\nTelecommunications\nTelecommunications\n\n\n520\nTelephone & Data Systems\n5044.0\n153.0\n9900\nTelecommunications\nTelecommunications\n\n\n667\nWeis Markets\n3467.0\n98.4\n23000\nFood & Drug Stores\nFood and Drug Stores\n\n\n759\nHain Celestial Group\n2853.0\n67.4\n7825\nFood, Beverages & Tobacco\nFood Consumer Products\n\n\n774\nFossil Group\n2788.0\n-478.2\n12300\nApparel\nApparel\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n995\nSiteOne Landscape Supply\n1862.0\n54.6\n3664\nWholesalers\nWholesalers: Diversified\n\n\n996\nCharles River Laboratories Intl\n1858.0\n123.4\n11800\nHealth Care\nHealth Care: Pharmacy and Other Services\n\n\n997\nCoreLogic\n1851.0\n152.2\n5900\nBusiness Services\nFinancial Data Services\n\n\n998\nEnsign Group\n1849.0\n40.5\n21301\nHealth Care\nHealth Care: Medical Facilities\n\n\n999\nHCP\n1848.0\n414.2\n190\nFinancials\nReal estate\n\n\n\n\n63 rows × 6 columns\n\n\n\n\nsectors.get_group(\"Energy\").head()\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nSector\nIndustry\n\n\n\n\n1\nExxon Mobil\n244363.0\n19710.0\n71200\nEnergy\nPetroleum Refining\n\n\n12\nChevron\n134533.0\n9195.0\n51900\nEnergy\nPetroleum Refining\n\n\n27\nPhillips 66\n91568.0\n5106.0\n14600\nEnergy\nPetroleum Refining\n\n\n30\nValero Energy\n88407.0\n4065.0\n10015\nEnergy\nPetroleum Refining\n\n\n40\nMarathon Petroleum\n67610.0\n3432.0\n43800\nEnergy\nPetroleum Refining"
  },
  {
    "objectID": "pandas/part2_applied_pandas.html#aggregate-operations",
    "href": "pandas/part2_applied_pandas.html#aggregate-operations",
    "title": "Pandas Part2",
    "section": "9.4 Aggregate operations",
    "text": "9.4 Aggregate operations\n\nsectors.sum().head()\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nIndustry\n\n\nSector\n\n\n\n\n\n\n\n\n\nAerospace & Defense\nBoeingUnited TechnologiesLockheed MartinGenera...\n383835.0\n26733.5\n1010124\nAerospace and DefenseAerospace and DefenseAero...\n\n\nApparel\nNikeVFPVHRalph LaurenHanesbrandsUnder ArmourLe...\n101157.3\n6350.7\n355699\nApparelApparelApparelApparelApparelApparelAppa...\n\n\nBusiness Services\nManpowerGroupVisaOmnicom GroupAramarkWaste Man...\n316090.0\n37179.2\n1593999\nTemporary HelpFinancial Data ServicesAdvertisi...\n\n\nChemicals\nDowDuPontSherwin-WilliamsPPG IndustriesMonsant...\n251151.0\n20475.0\n474020\nChemicalsChemicalsChemicalsChemicalsChemicalsC...\n\n\nEnergy\nExxon MobilChevronPhillips 66Valero EnergyMara...\n1543507.2\n85369.6\n981207\nPetroleum RefiningPetroleum RefiningPetroleum ...\n\n\n\n\n\n\n\n\nsectors.get_group(\"Aerospace & Defense\").Revenues.sum()\n\n383835.0\n\n\n\nsectors.Revenues\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object&gt;\n\n\n\nsectors.Revenues.sum().head()\n\nSector\nAerospace & Defense     383835.0\nApparel                 101157.3\nBusiness Services       316090.0\nChemicals               251151.0\nEnergy                 1543507.2\nName: Revenues, dtype: float64\n\n\n\nsectors.Profits.max().head()\n\nSector\nAerospace & Defense     8197.0\nApparel                 4240.0\nBusiness Services       6699.0\nChemicals               3000.4\nEnergy                 19710.0\nName: Profits, dtype: float64\n\n\n\naggregations = {\n    \"Revenues\": \"min\",\n    \"Profits\": \"max\",\n    \"Employees\": \"mean\"\n}\nsectors.agg(aggregations).head()\n\n\n\n\n\n\n\n\nRevenues\nProfits\nEmployees\n\n\nSector\n\n\n\n\n\n\n\nAerospace & Defense\n1877.0\n8197.0\n40404.960000\n\n\nApparel\n2350.0\n4240.0\n25407.071429\n\n\nBusiness Services\n1851.0\n6699.0\n30075.452830\n\n\nChemicals\n1925.0\n3000.4\n14364.242424\n\n\nEnergy\n1874.0\n19710.0\n9170.158879"
  },
  {
    "objectID": "pandas/part2_applied_pandas.html#applying-a-custom-operation-to-all-groups",
    "href": "pandas/part2_applied_pandas.html#applying-a-custom-operation-to-all-groups",
    "title": "Pandas Part2",
    "section": "9.5 Applying a custom operation to all groups",
    "text": "9.5 Applying a custom operation to all groups\n\nfortune.nlargest(n=5, columns=\"Profits\")\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nSector\nIndustry\n\n\n\n\n3\nApple\n229234.0\n48351.0\n123000\nTechnology\nComputers, Office Equipment\n\n\n2\nBerkshire Hathaway\n242137.0\n44940.0\n377000\nFinancials\nInsurance: Property and Casualty (Stock)\n\n\n15\nVerizon\n126034.0\n30101.0\n155400\nTelecommunications\nTelecommunications\n\n\n8\nAT&T\n160546.0\n29450.0\n254000\nTelecommunications\nTelecommunications\n\n\n19\nJPMorgan Chase\n113899.0\n24441.0\n252539\nFinancials\nCommercial Banks\n\n\n\n\n\n\n\n\ndef get_largest_row(df):\n    return df.nlargest(1, \"Revenues\")\n\n\nsectors.apply(get_largest_row).head()\n\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nSector\nIndustry\n\n\nSector\n\n\n\n\n\n\n\n\n\n\n\nAerospace & Defense\n26\nBoeing\n93392.0\n8197.0\n140800\nAerospace & Defense\nAerospace and Defense\n\n\nApparel\n88\nNike\n34350.0\n4240.0\n74400\nApparel\nApparel\n\n\nBusiness Services\n142\nManpowerGroup\n21034.0\n545.4\n29000\nBusiness Services\nTemporary Help\n\n\nChemicals\n46\nDowDuPont\n62683.0\n1460.0\n98000\nChemicals\nChemicals\n\n\nEnergy\n1\nExxon Mobil\n244363.0\n19710.0\n71200\nEnergy\nPetroleum Refining"
  },
  {
    "objectID": "pandas/part2_applied_pandas.html#grouping-by-multiple-columns",
    "href": "pandas/part2_applied_pandas.html#grouping-by-multiple-columns",
    "title": "Pandas Part2",
    "section": "9.6 Grouping by multiple columns",
    "text": "9.6 Grouping by multiple columns\n\nsector_and_industry = fortune.groupby(by = [\"Sector\", \"Industry\"])\n\n\nsector_and_industry.size()\n\nSector               Industry                                     \nAerospace & Defense  Aerospace and Defense                            25\nApparel              Apparel                                          14\nBusiness Services    Advertising, marketing                            2\n                     Diversified Outsourcing Services                 14\n                     Education                                         2\n                                                                      ..\nTransportation       Trucking, Truck Leasing                          11\nWholesalers          Wholesalers: Diversified                         24\n                     Wholesalers: Electronics and Office Equipment     8\n                     Wholesalers: Food and Grocery                     6\n                     Wholesalers: Health Care                          6\nLength: 82, dtype: int64\n\n\n\nsector_and_industry.get_group((\"Business Services\", \"Education\"))\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\nSector\nIndustry\n\n\n\n\n567\nLaureate Education\n4378.0\n91.5\n54500\nBusiness Services\nEducation\n\n\n810\nGraham Holdings\n2592.0\n302.0\n16153\nBusiness Services\nEducation\n\n\n\n\n\n\n\n\nsector_and_industry.sum().head()\n\n\n\n\n\n\n\n\n\nCompany\nRevenues\nProfits\nEmployees\n\n\nSector\nIndustry\n\n\n\n\n\n\n\n\nAerospace & Defense\nAerospace and Defense\nBoeingUnited TechnologiesLockheed MartinGenera...\n383835.0\n26733.5\n1010124\n\n\nApparel\nApparel\nNikeVFPVHRalph LaurenHanesbrandsUnder ArmourLe...\n101157.3\n6350.7\n355699\n\n\nBusiness Services\nAdvertising, marketing\nOmnicom GroupInterpublic Group\n23156.0\n1667.4\n127500\n\n\nDiversified Outsourcing Services\nAramarkADPConduentABM IndustriesCintasADTIron ...\n74175.0\n5043.7\n858600\n\n\nEducation\nLaureate EducationGraham Holdings\n6970.0\n393.5\n70653\n\n\n\n\n\n\n\n\nsector_and_industry[\"Revenues\"].mean().head(5)\n\nSector               Industry                        \nAerospace & Defense  Aerospace and Defense               15353.400000\nApparel              Apparel                              7225.521429\nBusiness Services    Advertising, marketing              11578.000000\n                     Diversified Outsourcing Services     5298.214286\n                     Education                            3485.000000\nName: Revenues, dtype: float64"
  },
  {
    "objectID": "pandas/part2_applied_pandas.html#coding-challenge",
    "href": "pandas/part2_applied_pandas.html#coding-challenge",
    "title": "Pandas Part2",
    "section": "9.7 Coding challenge",
    "text": "9.7 Coding challenge\n\ncereals = pd.read_csv(BASE / \"pandas-in-action/cereals.csv\")\ncereals.head()\n\n\n\n\n\n\n\n\nName\nManufacturer\nType\nCalories\nFiber\nSugars\n\n\n\n\n0\n100% Bran\nNabisco\nCold\n70\n10.0\n6\n\n\n1\n100% Natural Bran\nQuaker Oats\nCold\n120\n2.0\n8\n\n\n2\nAll-Bran\nKellogg's\nCold\n70\n9.0\n5\n\n\n3\nAll-Bran with Extra Fiber\nKellogg's\nCold\n50\n14.0\n0\n\n\n4\nAlmond Delight\nRalston Purina\nCold\n110\n1.0\n8\n\n\n\n\n\n\n\n\nmanufacturer = cereals.groupby(\"Manufacturer\")\n\n\nlen(manufacturer)\n\n7\n\n\n\nmanufacturer.size()\n\nManufacturer\nAmerican Home Food Products     1\nGeneral Mills                  22\nKellogg's                      23\nNabisco                         6\nPost                            9\nQuaker Oats                     8\nRalston Purina                  8\ndtype: int64\n\n\n\nmanufacturer.get_group(\"Nabisco\")\n\n\n\n\n\n\n\n\nName\nManufacturer\nType\nCalories\nFiber\nSugars\n\n\n\n\n0\n100% Bran\nNabisco\nCold\n70\n10.0\n6\n\n\n20\nCream of Wheat (Quick)\nNabisco\nHot\n100\n1.0\n0\n\n\n63\nShredded Wheat\nNabisco\nCold\n80\n3.0\n0\n\n\n64\nShredded Wheat 'n'Bran\nNabisco\nCold\n90\n4.0\n0\n\n\n65\nShredded Wheat spoon size\nNabisco\nCold\n90\n3.0\n0\n\n\n68\nStrawberry Fruit Wheats\nNabisco\nCold\n90\n3.0\n5\n\n\n\n\n\n\n\n\nmanufacturer[['Calories', 'Fiber', 'Sugars']].mean()\n\n\n\n\n\n\n\n\nCalories\nFiber\nSugars\n\n\nManufacturer\n\n\n\n\n\n\n\nAmerican Home Food Products\n100.000000\n0.000000\n3.000000\n\n\nGeneral Mills\n111.363636\n1.272727\n7.954545\n\n\nKellogg's\n108.695652\n2.739130\n7.565217\n\n\nNabisco\n86.666667\n4.000000\n1.833333\n\n\nPost\n108.888889\n2.777778\n8.777778\n\n\nQuaker Oats\n95.000000\n1.337500\n5.250000\n\n\nRalston Purina\n115.000000\n1.875000\n6.125000\n\n\n\n\n\n\n\n\nmanufacturer[\"Sugars\"].max()\n\nManufacturer\nAmerican Home Food Products     3\nGeneral Mills                  14\nKellogg's                      15\nNabisco                         6\nPost                           15\nQuaker Oats                    12\nRalston Purina                 11\nName: Sugars, dtype: int64\n\n\n\nmanufacturer.Fiber.min()\n\nManufacturer\nAmerican Home Food Products    0.0\nGeneral Mills                  0.0\nKellogg's                      0.0\nNabisco                        1.0\nPost                           0.0\nQuaker Oats                    0.0\nRalston Purina                 0.0\nName: Fiber, dtype: float64\n\n\n\nfrom pandas import DataFrame\n\n\ndef smallest_sugar_row(pd:DataFrame):\n    return pd.nsmallest(1, \"Sugars\")\n\nmanufacturer.apply(smallest_sugar_row).head()\n\n\n\n\n\n\n\n\n\nName\nManufacturer\nType\nCalories\nFiber\nSugars\n\n\nManufacturer\n\n\n\n\n\n\n\n\n\n\n\nAmerican Home Food Products\n43\nMaypo\nAmerican Home Food Products\nHot\n100\n0.0\n3\n\n\nGeneral Mills\n11\nCheerios\nGeneral Mills\nCold\n110\n2.0\n1\n\n\nKellogg's\n3\nAll-Bran with Extra Fiber\nKellogg's\nCold\n50\n14.0\n0\n\n\nNabisco\n20\nCream of Wheat (Quick)\nNabisco\nHot\n100\n1.0\n0\n\n\nPost\n33\nGrape-Nuts\nPost\nCold\n110\n3.0\n3"
  },
  {
    "objectID": "pandas/part1_core_pandas.html",
    "href": "pandas/part1_core_pandas.html",
    "title": "Pandas Part1",
    "section": "",
    "text": "Pandas DataFrame是一个二维数组，底层使用Numpy和array存储，Numpy使用C语言编写，运行速度很快\nSpark和Pandas都可以集成SQL能力，但他们支持的SQL规范不一致，为了保持统一，需要做数据转换\nPySpark DataFram转Pandas DataFrame使用toPandas()方法\nPandas DataFrame转PySpark DataFrame，使用sqlContext.createDataFrame(pdf)\n\n\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#pyspark-vs-pandas-vs-numpy",
    "href": "pandas/part1_core_pandas.html#pyspark-vs-pandas-vs-numpy",
    "title": "Pandas Part1",
    "section": "",
    "text": "Pandas DataFrame是一个二维数组，底层使用Numpy和array存储，Numpy使用C语言编写，运行速度很快\nSpark和Pandas都可以集成SQL能力，但他们支持的SQL规范不一致，为了保持统一，需要做数据转换\nPySpark DataFram转Pandas DataFrame使用toPandas()方法\nPandas DataFrame转PySpark DataFrame，使用sqlContext.createDataFrame(pdf)\n\n\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#创建数据",
    "href": "pandas/part1_core_pandas.html#创建数据",
    "title": "Pandas Part1",
    "section": "创建数据",
    "text": "创建数据\npandas支持创建单列数据 Series和多列数据 DataFrame\n\nSeries 数据\nSeries数据是一维数据，Index可以是一个，也可以有多个\n\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\ns\n\n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64\n\n\n使用boolean array过滤Series数据，应用数学函数\n\nprint(s[s&lt;5])\nprint(s*2)\nprint(np.exp(s))\n\n0    1.0\n1    3.0\ndtype: float64\n0     2.0\n1     6.0\n2    10.0\n3     NaN\n4    12.0\n5    16.0\ndtype: float64\n0       2.718282\n1      20.085537\n2     148.413159\n3            NaN\n4     403.428793\n5    2980.957987\ndtype: float64\n\n\n通过values和index属性，可以获取数组的值和索引对象\n\nprint(3 in s)\nprint(s.values)\nprint(s.index)\n\nTrue\n[ 1.  3.  5. nan  6.  8.]\nRangeIndex(start=0, stop=6, step=1)\n\n\n可以重新指定index的指\n\ns.index = ['a','b','c','d','e','f']\ns\n\na    1.0\nb    3.0\nc    5.0\nd    NaN\ne    6.0\nf    8.0\ndtype: float64\n\n\n可以把Series当做一个固定长度的、排序的字典, 可以传入一个dict来创建Series\n\nsdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\nobj3 = pd.Series(sdata)\nobj3\n\nOhio      35000\nTexas     71000\nOregon    16000\nUtah       5000\ndtype: int64\n\n\n也可以指定自己的索引来获取指定的数据\n\nstates = ['California', 'Ohio', 'Oregon', 'Texas']\nobj4 = pd.Series(sdata, index=states) # 只会获取索引在sdata中发现的数据\nobj4\n\nCalifornia        NaN\nOhio          35000.0\nOregon        16000.0\nTexas         71000.0\ndtype: float64\n\n\n查看空数据,可以使用pd的isnull方法，也可以直接调用seris的isnull方法\n\npd.isnull(obj4)\n\nCalifornia     True\nOhio          False\nOregon        False\nTexas         False\ndtype: bool\n\n\n\nobj4.notnull()\n\nCalifornia    False\nOhio           True\nOregon         True\nTexas          True\ndtype: bool\n\n\n\nobj3 + obj4\n\nCalifornia         NaN\nOhio           70000.0\nOregon         32000.0\nTexas         142000.0\nUtah               NaN\ndtype: float64\n\n\nSeries对象和它的index对象都有一个name属性\n\nobj4.name = 'population'\nobj4.index.name = 'state'\nobj4\n\nstate\nCalifornia        NaN\nOhio          35000.0\nOregon        16000.0\nTexas         71000.0\nName: population, dtype: float64\n\n\n创建Series的同时，可以指定index\n\ningredients = pd.Series(['4 cups', '1 cup', '2 large', '1 can'], index=['Flour', 'Milk', 'Eggs', 'Spam'], name='Dinner')\ningredients\n\nFlour     4 cups\nMilk       1 cup\nEggs     2 large\nSpam       1 can\nName: Dinner, dtype: object\n\n\n使用index中的labels来访问一个值，或者一组值\n\nprint(ingredients['Flour'])\nprint(ingredients[['Flour','Milk']])\n\n4 cups\nFlour    4 cups\nMilk      1 cup\nName: Dinner, dtype: object\n\n\nSeries 和 DataFrame数据转换\n\nimport pandas as pd\n\ndata_list = [(('一级科员', '0-20'), 0), (('一级科员', '21-30'), 14), (('一级科员', '31-40'), 10), \n             (('一级科员', '41-50'), 5), (('一级科员', '51-60'), 6), (('一级科员', '60+'), 0),\n             (('三级科员', '0-20'), 0), (('三级科员', '21-30'), 11), (('三级科员', '31-40'), 8)]\ndata = pd.Series([x[1] for x in data_list], index=[x[0] for x in data_list])\ndata\n\n(一级科员, 0-20)      0\n(一级科员, 21-30)    14\n(一级科员, 31-40)    10\n(一级科员, 41-50)     5\n(一级科员, 51-60)     6\n(一级科员, 60+)       0\n(三级科员, 0-20)      0\n(三级科员, 21-30)    11\n(三级科员, 31-40)     8\ndtype: int64\n\n\n\ndf = data.reset_index() # 要将组合Index的Series转换成DataFrame，需要重置索引\ndf.columns = ['Combination', 'Value']\ndf[['Rank', 'Age']] = df['Combination'].apply(pd.Series) # 将组合列拆分成两列\ndf\n\n\n\n\n\n\n\n\nCombination\nValue\nRank\nAge\n\n\n\n\n0\n(一级科员, 0-20)\n0\n一级科员\n0-20\n\n\n1\n(一级科员, 21-30)\n14\n一级科员\n21-30\n\n\n2\n(一级科员, 31-40)\n10\n一级科员\n31-40\n\n\n3\n(一级科员, 41-50)\n5\n一级科员\n41-50\n\n\n4\n(一级科员, 51-60)\n6\n一级科员\n51-60\n\n\n5\n(一级科员, 60+)\n0\n一级科员\n60+\n\n\n6\n(三级科员, 0-20)\n0\n三级科员\n0-20\n\n\n7\n(三级科员, 21-30)\n11\n三级科员\n21-30\n\n\n8\n(三级科员, 31-40)\n8\n三级科员\n31-40\n\n\n\n\n\n\n\n\ndata.dtypes\n\ndtype('int64')\n\n\n\n\nDataFrame数据\n通过传入一个Numpy数组来创建DataFrame，使用date_range来创建索引数据\n注意返回的数据里面有一个freq对象，这个表示时间类数据的频率，D表示日历日频率，常见的有： - B 工作日频率 - C 自定义工作日频率 - D 日历日频率 - W 周频率 - M 月末频率 - SM 半月结束频率 - BM 营业月结束频率\n\nimport pandas as pd\nimport numpy as np\n\ndates = pd.date_range(\"20130101\", periods=6)\ndates\n\nDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n               '2013-01-05', '2013-01-06'],\n              dtype='datetime64[ns]', freq='D')\n\n\n\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(\"ABCD\"))\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2013-01-01\n0.737109\n-1.512517\n-1.889726\n-0.903838\n\n\n2013-01-02\n1.012142\n2.329710\n0.269707\n0.569603\n\n\n2013-01-03\n0.071112\n-1.318172\n-0.582907\n0.134474\n\n\n2013-01-04\n0.562635\n-0.063299\n0.927822\n0.071969\n\n\n2013-01-05\n-0.595525\n1.406538\n0.419552\n0.038937\n\n\n2013-01-06\n-0.316271\n-0.015060\n1.235434\n0.041634\n\n\n\n\n\n\n\n\ndf.sum(axis=1)\n\n2013-01-01   -3.568972\n2013-01-02    4.181162\n2013-01-03   -1.695493\n2013-01-04    1.499127\n2013-01-05    1.269501\n2013-01-06    0.945737\nFreq: D, dtype: float64\n\n\n\ndf.gt(0).sum(axis=1)\n\n2013-01-01    1\n2013-01-02    4\n2013-01-03    2\n2013-01-04    3\n2013-01-05    3\n2013-01-06    2\nFreq: D, dtype: int64\n\n\n查看数据情况(行数,列数)\n\ndf.shape\n\n(6, 4)\n\n\n\npro = np.product(df.shape)\npro\n\n24\n\n\n\na = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\ndf\n\n\n\n\n\n\n\n\none\ntwo\nthree\n\n\n\n\n0\na\n1.2\n4.2\n\n\n1\nb\n70\n0.03\n\n\n2\nx\n5\n0\n\n\n\n\n\n\n\n通过分隔字符串创建DataFrame\n\nimport pandas as pd\n# 使用给定的示例数据\ndata = '''伍峰    三级科员    2000-10-8   2004-10-13  2006-10-13\n何健  四级科员    1978-5-6    2018-6-13   2020-6-13\n湛俊  二级科员    1967-9-20   2004-12-11  2008-12-11\n柏强  二级科员    1975-7-15   2019-3-1    2022-3-1\n孟俊  一级科员    1966-2-14   1992-10-10  1996-10-10'''\n\ndf = pd.DataFrame([x.split('\\t') for x in data.split('\\n')], columns=['name', 'job', 'birth_date', 'entry_date', 'last_promotion_date'])\ndf.head()\n\n\n\n\n\n\n\n\nname\njob\nbirth_date\nentry_date\nlast_promotion_date\n\n\n\n\n0\n伍峰\n三级科员\n2000-10-8\n2004-10-13\n2006-10-13\n\n\n1\n何健\n四级科员\n1978-5-6\n2018-6-13\n2020-6-13\n\n\n2\n湛俊\n二级科员\n1967-9-20\n2004-12-11\n2008-12-11\n\n\n3\n柏强\n二级科员\n1975-7-15\n2019-3-1\n2022-3-1\n\n\n4\n孟俊\n一级科员\n1966-2-14\n1992-10-10\n1996-10-10\n\n\n\n\n\n\n\n通过传入字典对象来创建DataFrame\n\npeople = pd.DataFrame({'Name':['a','b'],'Age':[18, 22]}, index=[0,1])\npeople\n\n\n\n\n\n\n\n\nName\nAge\n\n\n\n\n0\na\n18\n\n\n1\nb\n22\n\n\n\n\n\n\n\n\nloc = pd.DataFrame({'Location':['四川1','重庆']}, index=[0,1])\np2 = people.join(loc)\np2\n\n\n\n\n\n\n\n\nName\nAge\nLocation\n\n\n\n\n0\na\n18\n四川1\n\n\n1\nb\n22\n重庆\n\n\n\n\n\n\n\n\ndf2 = pd.DataFrame({\n    \"A\": 1.0,\n    \"B\": pd.Timestamp(\"20130102\"),\n    \"C\": pd.Series(1, index=list(range(4)), dtype=\"float32\"),\n    \"D\": np.array([3]*4, dtype=\"int32\"),\n    \"E\": pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n    \"F\": \"foo\",\n})\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\n0\n1.0\n2013-01-02\n1.0\n3\ntest\nfoo\n\n\n1\n1.0\n2013-01-02\n1.0\n3\ntrain\nfoo\n\n\n2\n1.0\n2013-01-02\n1.0\n3\ntest\nfoo\n\n\n3\n1.0\n2013-01-02\n1.0\n3\ntrain\nfoo\n\n\n\n\n\n\n\n\ndf2.dtypes\n\nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n\n\n\ndf2.index\n\nIndex([0, 1, 2, 3], dtype='int64')\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nname\njob\nbirth_date\nentry_date\nlast_promotion_date\n\n\n\n\ncount\n5\n5\n5\n5\n5\n\n\nunique\n5\n4\n5\n5\n5\n\n\ntop\n伍峰\n二级科员\n2000-10-8\n2004-10-13\n2006-10-13\n\n\nfreq\n1\n2\n1\n1\n1\n\n\n\n\n\n\n\n转置数据,行列颠倒\n\ndf.T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nname\n伍峰\n何健\n湛俊\n柏强\n孟俊\n\n\njob\n三级科员\n四级科员\n二级科员\n二级科员\n一级科员\n\n\nbirth_date\n2000-10-8\n1978-5-6\n1967-9-20\n1975-7-15\n1966-2-14\n\n\nentry_date\n2004-10-13\n2018-6-13\n2004-12-11\n2019-3-1\n1992-10-10\n\n\nlast_promotion_date\n2006-10-13\n2020-6-13\n2008-12-11\n2022-3-1\n1996-10-10\n\n\n\n\n\n\n\n对列进行排序\n\ndf.sort_index(axis=1, ascending=False)\n\n\n\n\n\n\n\n\nname\nlast_promotion_date\njob\nentry_date\nbirth_date\n\n\n\n\n0\n伍峰\n2006-10-13\n三级科员\n2004-10-13\n2000-10-8\n\n\n1\n何健\n2020-6-13\n四级科员\n2018-6-13\n1978-5-6\n\n\n2\n湛俊\n2008-12-11\n二级科员\n2004-12-11\n1967-9-20\n\n\n3\n柏强\n2022-3-1\n二级科员\n2019-3-1\n1975-7-15\n\n\n4\n孟俊\n1996-10-10\n一级科员\n1992-10-10\n1966-2-14\n\n\n\n\n\n\n\n对值排序\n删除数据, 在删除时可以指定axis, axis=0是按行删除,axis=1是按列删除\n\nimport pandas as pd\n\n# 创建一个包含五行的 DataFrame\ndata = {'A': [1, 2, 3, 4, 5], 'B': ['a', pd.NA, 'c', 'd', 'e'], 'C': [True,True,True,True,False], 'D': [1.1, 2.3, pd.NA, 4.5, 5.6]}\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1\na\nTrue\n1.1\n\n\n1\n2\n&lt;NA&gt;\nTrue\n2.3\n\n\n2\n3\nc\nTrue\n&lt;NA&gt;\n\n\n3\n4\nd\nTrue\n4.5\n\n\n4\n5\ne\nFalse\n5.6\n\n\n\n\n\n\n\n\n# 删除第三行\ndrop_row_df = df.drop(2)\ndrop_row_df\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1\na\nTrue\n1.1\n\n\n1\n2\n&lt;NA&gt;\nTrue\n2.3\n\n\n3\n4\nd\nTrue\n4.5\n\n\n4\n5\ne\nFalse\n5.6\n\n\n\n\n\n\n\n\n# 删除空行\ndrop_null_row = df.dropna(axis=0)\ndrop_null_row\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1\na\nTrue\n1.1\n\n\n3\n4\nd\nTrue\n4.5\n\n\n4\n5\ne\nFalse\n5.6\n\n\n\n\n\n\n\n\n# 只删除指定的，包含空列的行\ndrop_specified_row = df.dropna(axis=0, subset=['B'])\ndrop_specified_row\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1\na\nTrue\n1.1\n\n\n2\n3\nc\nTrue\n&lt;NA&gt;\n\n\n3\n4\nd\nTrue\n4.5\n\n\n4\n5\ne\nFalse\n5.6\n\n\n\n\n\n\n\n\n# 删除空列\ndrop_null_col = df.dropna(axis=1)\ndrop_null_col\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\n0\n1\nTrue\n\n\n1\n2\nTrue\n\n\n2\n3\nTrue\n\n\n3\n4\nTrue\n\n\n4\n5\nFalse\n\n\n\n\n\n\n\n\n# 直接修改原数据\ndf.dropna(inplace=True)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n1\na\nTrue\n1.1\n\n\n3\n4\nd\nTrue\n4.5\n\n\n4\n5\ne\nFalse\n5.6"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#转换数据",
    "href": "pandas/part1_core_pandas.html#转换数据",
    "title": "Pandas Part1",
    "section": "转换数据",
    "text": "转换数据\n使用pivot将行转换为列, 使用mellt将数据从列转换为行\npivot()方法可以接受以下参数：\n\nindex：用作新DataFrame的行索引的列名或列名列表。\ncolumns：用作新DataFrame的列索引的列名或列名列表。\nvalues：用于填充新DataFrame的列的列名或列名列表。如果未指定，则使用所有其他列。\naggfunc：用于聚合重复值的函数，例如“sum”或“mean”。\n\n\nimport pandas as pd\n\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],\n    'Year': [2016, 2016, 2017, 2017, 2018],\n    'Salary': [50000, 55000, 60000, 65000, 70000]\n}\n\ndf = pd.DataFrame(data)\nprint('index:', df.index)\nprint('columns:',df.columns)\ndf.head()\n\nindex: RangeIndex(start=0, stop=5, step=1)\ncolumns: Index(['Name', 'Year', 'Salary'], dtype='object')\n\n\n\n\n\n\n\n\n\nName\nYear\nSalary\n\n\n\n\n0\nAlice\n2016\n50000\n\n\n1\nBob\n2016\n55000\n\n\n2\nCharlie\n2017\n60000\n\n\n3\nDavid\n2017\n65000\n\n\n4\nEmily\n2018\n70000\n\n\n\n\n\n\n\n\npivot_df = df.pivot(index='Year', columns='Name', values='Salary')\nprint('index:', pivot_df.index)\nprint('columns:',pivot_df.columns)\npivot_df.head()\n\nindex: Index([2016, 2017, 2018], dtype='int64', name='Year')\ncolumns: Index(['Alice', 'Bob', 'Charlie', 'David', 'Emily'], dtype='object', name='Name')\n\n\n\n\n\n\n\n\nName\nAlice\nBob\nCharlie\nDavid\nEmily\n\n\nYear\n\n\n\n\n\n\n\n\n\n2016\n50000.0\n55000.0\nNaN\nNaN\nNaN\n\n\n2017\nNaN\nNaN\n60000.0\n65000.0\nNaN\n\n\n2018\nNaN\nNaN\nNaN\nNaN\n70000.0\n\n\n\n\n\n\n\n\nreset_df = pivot_df.reset_index()\nreset_df.index.name='AutoRangeIndex'\nprint('index:', reset_df.index)\nprint('columns:',reset_df.columns)\nreset_df.head()\n\nindex: RangeIndex(start=0, stop=3, step=1, name='AutoRangeIndex')\ncolumns: Index(['Year', 'Alice', 'Bob', 'Charlie', 'David', 'Emily'], dtype='object', name='Name')\n\n\n\n\n\n\n\n\nName\nYear\nAlice\nBob\nCharlie\nDavid\nEmily\n\n\nAutoRangeIndex\n\n\n\n\n\n\n\n\n\n\n0\n2016\n50000.0\n55000.0\nNaN\nNaN\nNaN\n\n\n1\n2017\nNaN\nNaN\n60000.0\n65000.0\nNaN\n\n\n2\n2018\nNaN\nNaN\nNaN\nNaN\n70000.0\n\n\n\n\n\n\n\n\nmelt_df = reset_df.melt(id_vars='Year', var_name='Name1', value_name='Salary')\nprint('index:', melt_df.index)\nprint('columns:',melt_df.columns)\nmelt_df.head()\n\nindex: RangeIndex(start=0, stop=15, step=1)\ncolumns: Index(['Year', 'Name1', 'Salary'], dtype='object')\n\n\n\n\n\n\n\n\n\nYear\nName1\nSalary\n\n\n\n\n0\n2016\nAlice\n50000.0\n\n\n1\n2017\nAlice\nNaN\n\n\n2\n2018\nAlice\nNaN\n\n\n3\n2016\nBob\n55000.0\n\n\n4\n2017\nBob\nNaN\n\n\n\n\n\n\n\n转换指定的列\n\nimport pandas as pd\n\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],\n    'Year': [2016, 2016, 2017, 2017, 2018],\n    'Salary': [50000, 55000, 60000, 65000, 70000],\n    'Bonus': [1000, 2000, 3000, 4000, 5000]\n}\n\ndf = pd.DataFrame(data)\n\n\nmelt_df = df.melt(id_vars=['Name', 'Year'], value_vars=['Salary'], var_name='Metric', value_name='Amount')\nmelt_df.head(50)\n\n\n\n\n\n\n\n\nName\nYear\nMetric\nAmount\n\n\n\n\n0\nAlice\n2016\nSalary\n50000\n\n\n1\nBob\n2016\nSalary\n55000\n\n\n2\nCharlie\n2017\nSalary\n60000\n\n\n3\nDavid\n2017\nSalary\n65000\n\n\n4\nEmily\n2018\nSalary\n70000"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#数据选择",
    "href": "pandas/part1_core_pandas.html#数据选择",
    "title": "Pandas Part1",
    "section": "数据选择",
    "text": "数据选择\n使用布尔索引和使用 loc 方法都可以根据条件获取行，但它们在内存占用方面有一些区别。\n\n布尔索引：使用布尔索引可以直接筛选出满足条件的行，返回一个新的 DataFrame，其中只包含符合条件的行。这种方法不会复制原始数据，因此在内存占用方面相对较小。它是一种有效的方法，特别适用于大型数据集。\n\n# 示例条件：获取 'column_name' 列值大于 10 的行\ncondition = df['column_name'] &gt; 10\nfiltered_df = df[condition]\n\nloc 方法：loc 方法可以根据标签进行行选择，并返回一个新的 DataFrame。它复制了满足条件的行的数据，因此在内存占用方面可能比布尔索引稍微更大一些。如果数据集非常大，复制所有行的数据可能会占用更多内存。\n\n# 示例条件：获取 'column_name' 列值大于 10 的行\nfiltered_df = df.loc[df['column_name'] &gt; 10]\n总体而言，在大多数情况下，布尔索引会占用较少的内存，因为它直接从原始数据中选择和过滤行，而不复制数据。但是，如果你的数据集相对较小，使用 loc 方法也是一种方便和直观的方法，不会产生明显的内存问题。\n在选择方法时，请考虑数据集的大小、内存限制以及性能需求，并选择最适合你的具体情况的方法。\n选择单列，返回一个Series\n\nimport pandas as pd\n\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],\n    'Year': [2016, 2016, 2017, 2017, 2018],\n    'Salary': [50000, 55000, 60000, 65000, 70000],\n    'Bonus': [1000, 2000, 3000, 4000, 5000]\n}\n\ndf = pd.DataFrame(data)\n\n\ndf\n\n\n\n\n\n\n\n\nName\nYear\nSalary\nBonus\n\n\n\n\n0\nAlice\n2016\n50000\n1000\n\n\n1\nBob\n2016\n55000\n2000\n\n\n2\nCharlie\n2017\n60000\n3000\n\n\n3\nDavid\n2017\n65000\n4000\n\n\n4\nEmily\n2018\n70000\n5000\n\n\n\n\n\n\n\n\n如果要更改原始数据，请使用单一赋值操作（loc）：\n\n\ndf.loc[df['Year'] == 2016, 'Salary'] = 200\n\n\n如果想要一个副本，请确保强制让 Pandas 创建副本：\n\n\ndf_2016 = df.loc[df['Year'] == 2016].copy()\ndf_2016.loc[df_2016['Name']=='Alice', 'Bonus'] = 1234\ndf_2016.head()\n\n\n\n\n\n\n\n\nName\nYear\nSalary\nBonus\n\n\n\n\n0\nAlice\n2016\n200\n1234\n\n\n1\nBob\n2016\n200\n2000\n\n\n\n\n\n\n\n\n根据正则匹配\n使用str.contains包含字符，使用~是不包含\n\n\n返回指定列数据\n\ndf[0:3]\n\n\n\n\n\n\n\n\nName\nYear\nSalary\nBonus\n\n\n\n\n0\nAlice\n2016\n200\n1000\n\n\n1\nBob\n2016\n200\n2000\n\n\n2\nCharlie\n2017\n60000\n3000\n\n\n\n\n\n\n\n\n\n根据label获取数据\nDataFrame.loc 根据Label获取数据 支持的如下3种输入 * 一个label, e.g. 5 * list label, e.g. [‘a’,‘b’,‘c’] * slice object, e.g. ‘a’:‘f’\n\n\niloc 根据位置获取数据\ndf.iloc根据索引位置和列位置获取数据，注意两个参数都必须是位置，不能一个位置一个label * 第一个参数是行索引,可以是具体的数字，也可以是区间，比如0:3 * 第二个参数是列索引\n\ndf.iloc[3]\n\nName      David\nYear       2017\nSalary    65000\nBonus      4000\nName: 3, dtype: object\n\n\n\ndf.iloc[3:5, 0:2]\n\n\n\n\n\n\n\n\nName\nYear\n\n\n\n\n3\nDavid\n2017\n\n\n4\nEmily\n2018\n\n\n\n\n\n\n\n\ndf.iloc[[1,2,4],[0,2]]\n\n\n\n\n\n\n\n\nName\nSalary\n\n\n\n\n1\nBob\n200\n\n\n2\nCharlie\n60000\n\n\n4\nEmily\n70000\n\n\n\n\n\n\n\n\ndf.iloc[1:3, :]\n\n\n\n\n\n\n\n\nName\nYear\nSalary\nBonus\n\n\n\n\n1\nBob\n2016\n200\n2000\n\n\n2\nCharlie\n2017\n60000\n3000\n\n\n\n\n\n\n\n\ndf.iloc[:, 1:3]\n\n\n\n\n\n\n\n\nYear\nSalary\n\n\n\n\n0\n2016\n200\n\n\n1\n2016\n200\n\n\n2\n2017\n60000\n\n\n3\n2017\n65000\n\n\n4\n2018\n70000\n\n\n\n\n\n\n\n\ndf.iloc[1,1]\n\n2016\n\n\n\ndf.iat[1,1]\n\n2016"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#boolean索引",
    "href": "pandas/part1_core_pandas.html#boolean索引",
    "title": "Pandas Part1",
    "section": "Boolean索引",
    "text": "Boolean索引\n过滤数据\n\ndf[df[\"Year\"]&gt;2017]\n\n\n\n\n\n\n\n\nName\nYear\nSalary\nBonus\n\n\n\n\n4\nEmily\n2018\n70000\n5000\n\n\n\n\n\n\n\n使用isin()方法过滤数据\n\ndf2 = df.copy()\n\n\ns1 = pd.Series([1,2,3,4,5,6], index=pd.date_range(\"20130102\", periods=6))\n\n\ns1\n\n2013-01-02    1\n2013-01-03    2\n2013-01-04    3\n2013-01-05    4\n2013-01-06    5\n2013-01-07    6\nFreq: D, dtype: int64\n\n\n\ns2 = pd.Series([11,12,13,14,15,16,17])\ns2\n\n0    11\n1    12\n2    13\n3    14\n4    15\n5    16\n6    17\ndtype: int64"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#日期处理",
    "href": "pandas/part1_core_pandas.html#日期处理",
    "title": "Pandas Part1",
    "section": "日期处理",
    "text": "日期处理\n\nimport pandas as pd\n\n# 创建测试数据\ndf = pd.DataFrame({'date': ['1994.03', '2020.02', '2020.03']})\n\n# 将字符串列转换为datetime列\ndf['date'] = pd.to_datetime(df['date'], format='%Y.%m')\n\n# 打印转换后的结果\nprint(df)\n\n        date\n0 1994-03-01\n1 2020-02-01\n2 2020-03-01\n\n\n设置值\n\n# 根据label设置\ndf.at[dates[0],\"A\"] = 0\ndf.loc[dates[0]]\n\ndate    NaT\nA       0.0\nName: 2013-01-01 00:00:00, dtype: object\n\n\n\n# 根据位置设置\ndf.iat[0,1]=0\ndf.iloc[0,:]\n\ndate    1994-03-01 00:00:00\nA                       0.0\nName: 0, dtype: object\n\n\n\n# 设置一个数组\ndf.loc[:,\"D\"] = np.array([5]*len(df))\n\n\ndf.D\n\n0                      5\n1                      5\n2                      5\n2013-01-01 00:00:00    5\nName: D, dtype: int64"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#数据转换",
    "href": "pandas/part1_core_pandas.html#数据转换",
    "title": "Pandas Part1",
    "section": "数据转换",
    "text": "数据转换\n可以使用函数或者Mapping转换数据\n\ndata = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',\n                              'Pastrami', 'corned beef', 'Bacon',\n                              'pastrami', 'honey ham', 'nova lox'],\n                    'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\ndata\n\n\n\n\n\n\n\n\nfood\nounces\n\n\n\n\n0\nbacon\n4.0\n\n\n1\npulled pork\n3.0\n\n\n2\nbacon\n12.0\n\n\n3\nPastrami\n6.0\n\n\n4\ncorned beef\n7.5\n\n\n5\nBacon\n8.0\n\n\n6\npastrami\n3.0\n\n\n7\nhoney ham\n5.0\n\n\n8\nnova lox\n6.0\n\n\n\n\n\n\n\n\nlowercased = data.food.str.lower()\nlowercased\n\n0          bacon\n1    pulled pork\n2          bacon\n3       pastrami\n4    corned beef\n5          bacon\n6       pastrami\n7      honey ham\n8       nova lox\nName: food, dtype: object\n\n\n\nmeat_to_animal = {\n    'bacon': 'pig',\n    'pulled pork': 'pig',\n    'pastrami': 'cow',\n    'corned beef': 'cow',\n    'honey ham': 'pig',\n    'nova lox': 'salmon'\n}\ndata['animal'] = lowercased.map(meat_to_animal)\ndata\n\n\n\n\n\n\n\n\nfood\nounces\nanimal\n\n\n\n\n0\nbacon\n4.0\npig\n\n\n1\npulled pork\n3.0\npig\n\n\n2\nbacon\n12.0\npig\n\n\n3\nPastrami\n6.0\ncow\n\n\n4\ncorned beef\n7.5\ncow\n\n\n5\nBacon\n8.0\npig\n\n\n6\npastrami\n3.0\ncow\n\n\n7\nhoney ham\n5.0\npig\n\n\n8\nnova lox\n6.0\nsalmon\n\n\n\n\n\n\n\n\n替换值\n\ndata = pd.Series([1., -999., 2., -999., -1000., 3.])\ndata\n\n0       1.0\n1    -999.0\n2       2.0\n3    -999.0\n4   -1000.0\n5       3.0\ndtype: float64\n\n\n\ndata.replace(-999, np.nan)\n\n0       1.0\n1       NaN\n2       2.0\n3       NaN\n4   -1000.0\n5       3.0\ndtype: float64\n\n\n\ndata\n\n0       1.0\n1    -999.0\n2       2.0\n3    -999.0\n4   -1000.0\n5       3.0\ndtype: float64"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#数据缺失",
    "href": "pandas/part1_core_pandas.html#数据缺失",
    "title": "Pandas Part1",
    "section": "数据缺失",
    "text": "数据缺失\npandas使用np.nan来表示数据缺失\n缺失数据处理 对空值的处理有3种办法 1. 去掉包含空值的列 2. 给空值赋值为平均值 3. 赋值的同时，添加一个标明数值缺失的属性\n\n给空值赋值为平均值\n\nfrom sklearn.impute import SimpleImputer\n\ndf_si = pd.DataFrame({'A': [1, np.nan, 2, 3, 4], 'B': [2, 3, 4, 5, 6]})\nmy_imputer = SimpleImputer()\n\nimputed_df_si = pd.DataFrame(my_imputer.fit_transform(df_si))\nimputed_df_si\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.0\n2.0\n\n\n1\n2.5\n3.0\n\n\n2\n2.0\n4.0\n\n\n3\n3.0\n5.0\n\n\n4\n4.0\n6.0\n\n\n\n\n\n\n\n\nimputed_df_si_2 = pd.DataFrame(my_imputer.transform(df_si))\nimputed_df_si_2\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.0\n2.0\n\n\n1\n2.5\n3.0\n\n\n2\n2.0\n4.0\n\n\n3\n3.0\n5.0\n\n\n4\n4.0\n6.0\n\n\n\n\n\n\n\nReindexing 可以根据列进行数据变化，返回一个数据的拷贝\n\ndf1 = df.reindex(index=dates[0:4], columns=list(df.columns)+[\"E\"])\ndf1.loc[dates[0]:dates[1], \"E\"] = 1\ndf1\n\n\n\n\n\n\n\n\ndate\nA\nD\nE\n\n\n\n\n2013-01-01\nNaT\n0.0\n5.0\n1.0\n\n\n2013-01-02\nNaT\nNaN\nNaN\n1.0\n\n\n2013-01-03\nNaT\nNaN\nNaN\nNaN\n\n\n2013-01-04\nNaT\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndf1.at[\"20130102\",\"G\"]=2\n\n\ndf1\n\n\n\n\n\n\n\n\ndate\nA\nD\nE\nG\n\n\n\n\n2013-01-01\nNaT\n0.0\n5.0\n1.0\nNaN\n\n\n2013-01-02\nNaT\nNaN\nNaN\n1.0\n2.0\n\n\n2013-01-03\nNaT\nNaN\nNaN\nNaN\nNaN\n\n\n2013-01-04\nNaT\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nDataFrame.dropna()删处任何办好空数据的列，返回删除后的数据，不会改变原数据\n\ndf1.dropna(how=\"any\")\n\n\n\n\n\n\n\n\ndate\nA\nD\nE\nG\n\n\n\n\n\n\n\n\n\n\ndf1\n\n\n\n\n\n\n\n\ndate\nA\nD\nE\nG\n\n\n\n\n2013-01-01\nNaT\n0.0\n5.0\n1.0\nNaN\n\n\n2013-01-02\nNaT\nNaN\nNaN\n1.0\n2.0\n\n\n2013-01-03\nNaT\nNaN\nNaN\nNaN\nNaN\n\n\n2013-01-04\nNaT\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nDataFrame.fillna填充缺失的数据\n\ndf1.fillna(value=5)\n\n\n\n\n\n\n\n\ndate\nA\nD\nE\nG\n\n\n\n\n2013-01-01\n5\n0.0\n5.0\n1.0\n5.0\n\n\n2013-01-02\n5\n5.0\n5.0\n1.0\n2.0\n\n\n2013-01-03\n5\n5.0\n5.0\n5.0\n5.0\n\n\n2013-01-04\n5\n5.0\n5.0\n5.0\n5.0\n\n\n\n\n\n\n\nisna()获取nan的boolean数据\n\npd.isna(df1)\n\n\n\n\n\n\n\n\ndate\nA\nD\nE\nG\n\n\n\n\n2013-01-01\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n2013-01-02\nTrue\nTrue\nTrue\nFalse\nFalse\n\n\n2013-01-03\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n2013-01-04\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\n\ns = pd.Series([\"8\", 6, \"7.5\", 3, \"0.9\"]) # mixed string and numeric values\ns.dtypes\n\ndtype('O')\n\n\n\npd.to_numeric(s) # convert everything to float values\ns.dtypes\n\ndtype('O')"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#排序",
    "href": "pandas/part1_core_pandas.html#排序",
    "title": "Pandas Part1",
    "section": "排序",
    "text": "排序\n\ndf = pd.DataFrame({\n    'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n    'col2': [2, 1, 9, 8, 7, 4],\n    'col3': [0, 1, 9, 4, 2, 3],\n    'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n})\ndf.sort_values(by=['col1','col2'], ascending=True)\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\ncol4\n\n\n\n\n1\nA\n1\n1\nB\n\n\n0\nA\n2\n0\na\n\n\n2\nB\n9\n9\nc\n\n\n5\nC\n4\n3\nF\n\n\n4\nD\n7\n2\ne\n\n\n3\nNaN\n8\n4\nD\n\n\n\n\n\n\n\n要恢复排序可以使用reset_index\n\ndf.reset_index(drop=True)\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\ncol4\n\n\n\n\n0\nA\n2\n0\na\n\n\n1\nA\n1\n1\nB\n\n\n2\nB\n9\n9\nc\n\n\n3\nNaN\n8\n4\nD\n\n\n4\nD\n7\n2\ne\n\n\n5\nC\n4\n3\nF"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#索引",
    "href": "pandas/part1_core_pandas.html#索引",
    "title": "Pandas Part1",
    "section": "索引",
    "text": "索引\n\ndf = pd.DataFrame([('bird', 389.0),\n                   ('bird', 24.0),\n                   ('mammal', 80.5),\n                   ('mammal', np.nan)],\n                  index=['falcon', 'parrot', 'lion', 'monkey'],\n                  columns=('class', 'max_speed'))\ndf\n\n\n\n\n\n\n\n\nclass\nmax_speed\n\n\n\n\nfalcon\nbird\n389.0\n\n\nparrot\nbird\n24.0\n\n\nlion\nmammal\n80.5\n\n\nmonkey\nmammal\nNaN\n\n\n\n\n\n\n\n当我们执行reset_index后，旧的index会作用一列添加到数据中\n\ndf.reset_index()\n\n\n\n\n\n\n\n\nindex\nclass\nmax_speed\n\n\n\n\n0\nfalcon\nbird\n389.0\n\n\n1\nparrot\nbird\n24.0\n\n\n2\nlion\nmammal\n80.5\n\n\n3\nmonkey\nmammal\nNaN\n\n\n\n\n\n\n\n我们可以使用drop参数移除索引\n\ndf.reset_index(drop=True)\n\n\n\n\n\n\n\n\nclass\nmax_speed\n\n\n\n\n0\nbird\n389.0\n\n\n1\nbird\n24.0\n\n\n2\nmammal\n80.5\n\n\n3\nmammal\nNaN\n\n\n\n\n\n\n\n\nApply\nDataFrame.apply()应用一个用户自定义的函数到数据上\n\ndf\n\n\n\n\n\n\n\n\nclass\nmax_speed\n\n\n\n\nfalcon\nbird\n389.0\n\n\nparrot\nbird\n24.0\n\n\nlion\nmammal\n80.5\n\n\nmonkey\nmammal\nNaN\n\n\n\n\n\n\n\n\ndf.apply(np.cumsum)\n\n\n\n\n\n\n\n\nclass\nmax_speed\n\n\n\n\nfalcon\nbird\n389.0\n\n\nparrot\nbirdbird\n413.0\n\n\nlion\nbirdbirdmammal\n493.5\n\n\nmonkey\nbirdbirdmammalmammal\nNaN"
  },
  {
    "objectID": "pandas/part1_core_pandas.html#pandas函数",
    "href": "pandas/part1_core_pandas.html#pandas函数",
    "title": "Pandas Part1",
    "section": "Pandas函数",
    "text": "Pandas函数\nDataFrame.corr(method=‘pearson’, min_periods=1) 计算相关系数\n参数说明：\nmethod：可选值为{‘pearson’, ‘kendall’, ‘spearman’}\n\npearson：Pearson相关系数来衡量两个数据集合是否在一条线上面，即针对线性数据的相关系数计算，针对非线性数据便会有误差。\nkendall：用于反映分类变量相关性的指标，即针对无序序列的相关系数，非正太分布的数据\nspearman：非线性的，非正太分布的数据的相关系数\n\nmin_periods：样本最少的数据量\n返回值：各类型之间的相关系数DataFrame表格。\n两组数据间的相关性计算可以分为如下3种情况：\n\n数值数据与分类数据\n数值数据与数值数据\n分类数据与分类数据\n\n\n数值与数值的相关性\n\nimport pandas as pd\n \ndata = pd.DataFrame({'化妆品费': [30, 50, 120, 20, 70, 150, 50, 60, 80, 100],\n                     '置装费': [70, 80, 250, 50, 120, 300, 100, 150, 20, 180]})\nprint(data.corr()) # 计算所有的变量的两两相关性\nprint(data['化妆品费'].corr(data['置装费'])) # 只计算选择的两个变量的相关性\n\n          化妆品费       置装费\n化妆品费  1.000000  0.850918\n置装费   0.850918  1.000000\n0.8509180035311159\n\n\n\n\n数值与分类的相关性\n\n# 情况1：分类标签为数字\ndata = pd.DataFrame({'id': [3, 2, 1, 1, 2, 3, 2, 3, 1, 1, 2, 3, 1, 2, 1],\n                     'age': [27, 33, 16, 29, 32, 23, 25, 28, 22, 18, 26, 26, 15, 29, 26]})\nprint('pearson:', data['id'].corr(data['age']))\nprint('spearman', data['id'].corr(data['age'], method='spearman'))\n \n# 情况2：分类标签为字符串\ndata1 = pd.DataFrame({'id': ['c', 'b', 'a', 'a', 'b', 'c', 'b', 'c', 'a', 'a', 'b', 'c', 'a', 'b', 'a'],\n                     'age': [27, 33, 16, 29, 32, 23, 25, 28, 22, 18, 26, 26, 15, 29, 26]})\nprint('spearman', data1['id'].corr(data1['age'], method='spearman'))\n \n# 输出\n# pearson: 0.4465155114816965\n# spearman 0.4016086046008866\n# spearman 0.4016086046008866\n\npearson: 0.4465155114816965\nspearman 0.4016086046008866\nspearman 0.4016086046008866"
  },
  {
    "objectID": "sklearn_demo.html",
    "href": "sklearn_demo.html",
    "title": "Scikit-Learn",
    "section": "",
    "text": "有监督的机器学习算法涉及5个主要步骤:\n\n特征选择和标注训练样本的收集\n选择Performance Metrics\n选择学习算法并训练模型\n对模型性能进行评估\n更改算法的设置并调整模型"
  },
  {
    "objectID": "sklearn_demo.html#logistic-regression-and-conditional-probabillities",
    "href": "sklearn_demo.html#logistic-regression-and-conditional-probabillities",
    "title": "Scikit-Learn",
    "section": "Logistic regression and conditional probabillities",
    "text": "Logistic regression and conditional probabillities\n逻辑回归是一个非常容易实现的分类模型，并且在线性分隔时执行得很好。 它是工业界最广泛使用的分类模型之一。\n首先，先来看看激活函数Sigmoid的图形，参见： Figure 2\n\nimport matplotlib.pylab as plt\nimport numpy as np\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\nz = np.arange(-7, 7, 0.1)\nsigma_z = sigmoid(z)\nplt.plot(z, sigma_z)\nplt.axvline(0.0, color='k')\nplt.ylim(-0.1, 1.1)\nplt.xlabel('z')\nplt.ylabel('$\\sigma (z)$')\nplt.yticks([0.0, 0.5, 1.0])\nax = plt.gca()\nax.yaxis.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure 2: A plot of the logistic sigmoid function"
  },
  {
    "objectID": "sklearn_demo.html#模型训练",
    "href": "sklearn_demo.html#模型训练",
    "title": "Scikit-Learn",
    "section": "模型训练",
    "text": "模型训练\nscikit-learn 有两类回归模型，LogisticRegression和LinearRegression。 它们在功能和适用场景上有所不同。 逻辑回归只适用于二分类模型，所以只能使用0,1类别的数据\n\nLinearRegression（线性回归）: 用于建立线性回归模型，用于预测一个或多个连续数值型目标变量。\nLogisticRegression（逻辑回归）：用于建立逻辑回归模型，用于预测二分类或多分类问题。它使用逻辑函数（如sigmoid函数）将线性回归的输出映射到概率值，并根据阈值进行分类\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=100.0, solver='lbfgs',multi_class='ovr')\nlr.fit(X_train_std, y_train)\n\nLogisticRegression(C=100.0, multi_class='ovr')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=100.0, multi_class='ovr')\n\n\n看一下预测结果，参见: Figure 3\n\nplot_decision_regions(X_combined_std,\n                       y_combined,\n                       classifier=lr,\n                       test_idx=range(105, 150))\nplt.xlabel('Petal length [standardized]')\nplt.ylabel('Petal width [standardized]')\nplt.legend(loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure 3: Decision rigions for scikit-learn’s multi-class logistic regression model\n\n\n\n\n如果想看每个分类的预测概率，可以使用predict_proba\n\nlr.predict_proba(X_test_std[:3, :])\n\narray([[3.81527885e-09, 1.44792866e-01, 8.55207131e-01],\n       [8.34020679e-01, 1.65979321e-01, 3.25737138e-13],\n       [8.48831425e-01, 1.51168575e-01, 2.62277619e-14]])\n\n\n上面每一行的3个数代表每个分类的概率，所有的概率想加为1\n\nlr.predict_proba(X_test_std[:3, :]).sum(axis=1)\n\narray([1., 1., 1.])\n\n\n使用argmax函数来获取最大概率的列索引\n\nlr.predict_proba(X_test_std[:3, :]).argmax(axis=1)\n\narray([2, 0, 0])\n\n\n上面的代码和直接调用predict相同\n\nlr.predict(X_test_std[:3, :])\n\narray([2, 0, 0])"
  },
  {
    "objectID": "sklearn_demo.html#交叉验证cross-validation",
    "href": "sklearn_demo.html#交叉验证cross-validation",
    "title": "Scikit-Learn",
    "section": "交叉验证(Cross-Validation)",
    "text": "交叉验证(Cross-Validation)\n使用不同的数据作为验证集，当满足如下情况时，可以考虑使用这个方法 * 对于小数据量的情况，额外的训练工作消耗资源不多"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "数据分析",
    "section": "",
    "text": "如下是一些常用地理数据相关工具\n\n坐标拾取器\n街道数据\n地理空间数据云\nPOI数据获取\n百度经纬度定位\n\n安装包\npip install geopy\nconda install -c conda-forge geopandas\nimport pandas as pd"
  },
  {
    "objectID": "index.html#工具库",
    "href": "index.html#工具库",
    "title": "数据分析",
    "section": "",
    "text": "如下是一些常用地理数据相关工具\n\n坐标拾取器\n街道数据\n地理空间数据云\nPOI数据获取\n百度经纬度定位\n\n安装包\npip install geopy\nconda install -c conda-forge geopandas\nimport pandas as pd"
  },
  {
    "objectID": "index.html#决策树",
    "href": "index.html#决策树",
    "title": "数据分析",
    "section": "决策树",
    "text": "决策树\nfrom sklearn.model_selection import train_test_split\n\n# split data into training and validation data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n# Define model\n# final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = melbourne_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))"
  },
  {
    "objectID": "index.html#随机森林",
    "href": "index.html#随机森林",
    "title": "数据分析",
    "section": "随机森林",
    "text": "随机森林\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))"
  },
  {
    "objectID": "index.html#线性回归",
    "href": "index.html#线性回归",
    "title": "数据分析",
    "section": "线性回归",
    "text": "线性回归\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nreg.predict(np.array([[3, 5]]))\n结果偏差度\nfrom sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)\n分离训练数据和验证数据\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
  },
  {
    "objectID": "index.html#地图库",
    "href": "index.html#地图库",
    "title": "数据分析",
    "section": "地图库",
    "text": "地图库\n地图\nimport geopandas as gpd\n\n# 画国家的边境线\n# alpha 透明度\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nchina = world.loc[world.name.str.contains('China')]\nax = china.plot(figsize=(12,12), color='white', linestyle=':', edgecolor='gray')\npath.plot(ax=ax, color='red', markersize=30, alpha=0.4)\n# 根据Point画出Line\n过滤字符串\nworld.loc[world.name.str.contains('ina')]\n地图标记\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n分级统计图\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\n# Create a base map\nm_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a map\n# Create a map\ndef color_producer(magnitude):\n    if magnitude &gt; 6.5:\n        return 'red'\n    else:\n        return 'green'\n\nChoropleth(\n    geo_data=prefectures['geometry'].__geo_interface__,\n    data=stats['density'],\n    key_on=\"feature.id\",\n    fill_color='BuPu',\n    legend_name='Population density (per square kilometer)').add_to(m_4)\n\n# 画圆\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        popup=(\"{} ({})\").format(\n            earthquakes.iloc[i]['Magnitude'],\n            earthquakes.iloc[i]['DateTime'].year),\n        radius=earthquakes.iloc[i]['Magnitude']**5.5,\n        color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)"
  },
  {
    "objectID": "index.html#地图",
    "href": "index.html#地图",
    "title": "数据分析",
    "section": "地图",
    "text": "地图\n地图通过经纬度标明位置，其中经度正数为东经，负数为西经，纬度正数为北纬，负数为南纬\n地理空间数据常用的有几类： DEM,POI,OSM\nOSM 是Open Street Map的缩写，是一款由网络大众打造的免费开源、可编辑的地图服务。\nDEM数据,数字高程模型（Digital Elevation Model)，简称DEM\nPOI数据,POI是“Point of Interest”的缩写，可以翻译成“兴趣点”，也有些叫做“Point of Information”，即“信息点”。电子地图上一般用气泡图标来表示POI，像电子地图上的景点、政府机构、公司、商场、饭馆等，都是POI。\n\n关于坐标\n高德地图采用国家测绘地理信息局GCJ02坐标系（即俗称火星坐标系），百度采用自己的BD09坐标系，而国际来源地图大多采用WGS84坐标系，导致了多个来源的数据不能叠加在同一底图上，因此，需要在坐标之间相互转换\n\n\nGCJ02坐标系\n国家测绘地理信息局为了保密需要，按照特殊的算法将坐标进行非线性加密，加密后的坐标为GCJ02坐标系，又称为火星坐标系统。 国内正式发布的电子地图大多数采用GCJ02坐标系，如高德地图、腾讯地图、谷歌地图中国区域等。\n\n\n百度坐标系BD09\n百度坐标系是在GCJ02坐标系的基础上进行二次加密而来，目前主要由百度地图使用。\n\n\nWGS84坐标系（即EPSG:4326)\n一般从国际标准的GPS设备获取的坐标都是WGS84坐标，是国际地图提供商广泛使用的坐标系，如OpenStreetMap、ARCGIS 在线地图、必应地图等。\n默认单位是米\n\n另外还有 EPSG:3857(基于墨卡托坐标系)\n\n\n\n坐标转换\n不同坐标系的电子地图数据在叠加时会出现位置偏差，导致无法使用，需要进行坐标转换以消除偏差。有多种方法可以实现坐标之间的转换，例如直接编写算法实现；使用Web API实现或者使用现有的插件。\n\n\n距离计算\n经纬度属于球面坐标，而我们常规的距离是在平面维度上的，因此，在进行距离计算之前，首先需将球面坐标转换为平面坐标，这样之后才能进行平面距离的测算，计算出来的距离单位就是米了 ## 位置 火锅店，烤肉、麻辣烫与奶茶店 医院与煲汤店"
  },
  {
    "objectID": "seaborn_demo.html",
    "href": "seaborn_demo.html",
    "title": "Seaborn",
    "section": "",
    "text": "Seaborn 是基于Python且非常受欢迎的图形可视化库，在Matplotlib的基础上进行了更高级的封装，使得作图更加方便快捷。\n参见:\n\nSeaborn 官网\n\nSeaborn Galllery"
  },
  {
    "objectID": "seaborn_demo.html#介绍",
    "href": "seaborn_demo.html#介绍",
    "title": "Seaborn",
    "section": "",
    "text": "Seaborn 是基于Python且非常受欢迎的图形可视化库，在Matplotlib的基础上进行了更高级的封装，使得作图更加方便快捷。\n参见:\n\nSeaborn 官网\n\nSeaborn Galllery"
  },
  {
    "objectID": "seaborn_demo.html#lineplot折线图",
    "href": "seaborn_demo.html#lineplot折线图",
    "title": "Seaborn",
    "section": "lineplot（折线图）",
    "text": "lineplot（折线图）\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 创建数据\nx = [1, 2, 3, 4, 5]\ny = [4, 2, 1, 3, 5]\n\n# 创建画布\nsns.set_style(\"whitegrid\")\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# 绘制折线图\nsns.lineplot(x=x, y=y, ax=ax)\n\n# 设置标题和标签\nax.set_title(\"Example Line Plot\")\nax.set_xlabel(\"X-axis\")\nax.set_ylabel(\"Y-axis\")\n\n# 显示图形\nplt.show()\n\n\n\n\n\nflights = sns.load_dataset(\"flights\")\nflights.head()\n\n\n\n\n\n\n\n\nyear\nmonth\npassengers\n\n\n\n\n0\n1949\nJan\n112\n\n\n1\n1949\nFeb\n118\n\n\n2\n1949\nMar\n132\n\n\n3\n1949\nApr\n129\n\n\n4\n1949\nMay\n121\n\n\n\n\n\n\n\n\nmay_flights = flights.query(\"month == 'May'\")\nsns.lineplot(data=may_flights, x=\"year\", y=\"passengers\")\n\n&lt;Axes: xlabel='year', ylabel='passengers'&gt;\n\n\n\n\n\n\n使用hue展示多系列\n\nsns.lineplot(data=flights, x=\"year\", y=\"passengers\", hue=\"month\")\n\n&lt;Axes: xlabel='year', ylabel='passengers'&gt;\n\n\n\n\n\n\nflights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n\n\n\n\n\n\n\nmonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1949\n112\n118\n132\n129\n121\n135\n148\n148\n136\n119\n104\n118\n\n\n1950\n115\n126\n141\n135\n125\n149\n170\n170\n158\n133\n114\n140\n\n\n1951\n145\n150\n178\n163\n172\n178\n199\n199\n184\n162\n146\n166\n\n\n1952\n171\n180\n193\n181\n183\n218\n230\n242\n209\n191\n172\n194\n\n\n1953\n196\n196\n236\n235\n229\n243\n264\n272\n237\n211\n180\n201\n\n\n\n\n\n\n\n\nflights_wide.dtypes\n\nmonth\nJan    int64\nFeb    int64\nMar    int64\nApr    int64\nMay    int64\nJun    int64\nJul    int64\nAug    int64\nSep    int64\nOct    int64\nNov    int64\nDec    int64\ndtype: object\n\n\n\n\n展示多个字段多条线\n\n#create DataFrame\nimport pandas as pd\nimport seaborn as sns\nimport logging\nlogging.disable(logging.WARNING)\n\ndf = pd.DataFrame({'year': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'A': [10, 12, 14, 15, 15, 14, 13, 18],\n                   'B': [18, 18, 19, 14, 14, 11, 20, 28],\n                   'C': [5, 7, 7, 9, 12, 9, 9, 4],\n                   'D': [11, 8, 10, 6, 6, 5, 9, 12]})\n\n#plot sales of each store as a line\nsns.lineplot(data=df[['A', 'B', 'C', 'D']])\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "seaborn_demo.html#relplot关系图",
    "href": "seaborn_demo.html#relplot关系图",
    "title": "Seaborn",
    "section": "relplot(关系图)",
    "text": "relplot(关系图)\nrelplot()用于绘制关系型数据的图表。函数通常用于绘制散点图、线图、面积图等，可以根据需要选择使用不同的图表类型，默认使用散点图。\n\ncol=“variable”：指定要绘制的散点图按照variable列中不同的取值进行分组，并绘制多个子图，每个子图中的数据为该取值对应的行；\nfacet_kws=dict(sharex=False)：指定分组绘制的子图中，是否共享x轴坐标。这里设置为False表示不共享。\n\n\n使用%matplotlib inline告诉matplotlib库将图表直接输出到Notebook中。如果没有，就需要使用plt.show()显示图表\n\n\nimport seaborn as sns\nimport pandas as pd\n\n\ndata = {'x': [1, 2, 3, 4, 5], 'y': [3, 4, 2, 5, 1], 'hue': ['A', 'B', 'A', 'B', 'A']}\ndf = pd.DataFrame(data)\n\nsns.relplot(data=df, x='x', y='y', hue='hue')\n\n\n\n\n\nsns.relplot(data=df, x='x', y='y', col='hue', facet_kws=dict(sharex=False))"
  },
  {
    "objectID": "seaborn_demo.html#barplot-柱状图",
    "href": "seaborn_demo.html#barplot-柱状图",
    "title": "Seaborn",
    "section": "barplot (柱状图)",
    "text": "barplot (柱状图)\nbarplot() 函数默认对每个 x 值对应的 y 值进行聚合，并绘制该 y 值的平均值。\n\nsns.barplot(data=tips, x='sex', y='total_bill', hue='smoker')\n\n&lt;Axes: xlabel='sex', ylabel='total_bill'&gt;\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 创建示例数据\ndata = {'Category': ['A', 'B', 'C', 'D'],\n        'Value': [10, 5, 8, 3]}\n\n# 创建 barplot\nax = sns.barplot(x='Category', y='Value', data=data)\n\n# 自动显示标签\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')\n\n# 显示图形\nplt.show()"
  },
  {
    "objectID": "seaborn_demo.html#countbar统计图",
    "href": "seaborn_demo.html#countbar统计图",
    "title": "Seaborn",
    "section": "countbar(统计图)",
    "text": "countbar(统计图)\n\ndf\n\n\n\n\n\n\n\n\nx\ny\nhue\n\n\n\n\n0\n1\n3\nA\n\n\n1\n2\n4\nB\n\n\n2\n3\n2\nA\n\n\n3\n4\n5\nB\n\n\n4\n5\n1\nA\n\n\n\n\n\n\n\n\nsns.countplot(data=df, x=\"hue\").set(title=\"Histogram\");"
  },
  {
    "objectID": "seaborn_demo.html#catplot分类图",
    "href": "seaborn_demo.html#catplot分类图",
    "title": "Seaborn",
    "section": "catplot(分类图)",
    "text": "catplot(分类图)\ncatplot 可以绘制分类数据的图表，展示数据的分布情况和关系。\ncatplot函数的kind参数可以指定要绘制的图表类型，包括以下几种：\n\n“strip”：散点图，用于展示单个数据点的分布情况，可以使用jitter参数调整点的位置，避免重叠。\n“swarm”：蜂群图，与散点图类似，但会自动调整点的位置，避免重叠，因此适用于较大的数据集。\n“box”：箱线图，用于展示数据的分布情况，包括中位数、四分位数和异常值等信息。\n“boxen”: 箱线图，比box更细致,包含了更多的分位点和边缘线，可以更细致的展示数据的分布情况\n“violin”：小提琴图，与箱线图类似，但是可以展示更多的信息，包括数据的密度估计和分布情况。\n“bar”：条形图，用于展示类别变量的计数或平均值等统计信息。\n“count”：计数图，用于展示类别变量的计数信息。\n“point”：点图，与条形图类似，但是可以展示更多的信息，包括数据的分布情况和置信区间等。 除了kind参数，catplot函数还可以接收其他参数，如data、hue、col、row等，用于指定数据源、数据分类变量、子图列数和行数等。\n\n总之，Seaborn的catplot函数是一个非常实用的数据可视化工具，可以帮助我们更好地理解和展示分类数据的分布和关系。\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset(\"titanic\")\ng = sns.catplot(\n    data=df, x=\"who\", y=\"survived\", col=\"class\",\n    kind=\"bar\", height=4, aspect=.6,\n)\ng.set_axis_labels(\"\", \"Survival Rate\")\ng.set_xticklabels([\"Men\", \"Women\", \"Children\"], rotation=90)\n# g.set_titles(\"{col_name} {col_var}\")\ng.set(ylim=(0, 1))\ng.despine(left=True)\n\n\n\n\n\nsns.catplot(\n    data=df, x=\"who\", y=\"age\", \n    kind=\"boxen\", height=4, aspect=.6,\n)"
  },
  {
    "objectID": "fastai_core.html",
    "href": "fastai_core.html",
    "title": "Fastai Core",
    "section": "",
    "text": "import fastcore.all as fc\nfrom nbdev.showdoc import *"
  },
  {
    "objectID": "fastai_core.html#functions-on-functions",
    "href": "fastai_core.html#functions-on-functions",
    "title": "Fastai Core",
    "section": "Functions on Functions",
    "text": "Functions on Functions\nUtilities for functional programming or for defining, modifying, or debugging funtions.\n\nsource\n\nbind\n\n bind (func, *pargs, **pkwargs)\n\nSame as partial, except you can use arg0 arg1 etc param placeholders\nbind is the same as partial, but also allows you to reorder positional arguments using variable name(s) arg{i} where i refers to the zero-indexed positional argument. bind as implemented currently only supports reordering of up to the first 5 positional arguments.\nConsider the function myfunc below, which has 3 positional arguments. These arguments can be referenced as arg0, arg1, and arg1, respectively.\n\ndef myfn(a,b,c,d=1,e=2): return (a,b,c,d,e)\n\n\nfc.bind(myfn, fc.arg1,17, fc.arg0, e=3)(19,14)\n\n(14, 17, 19, 1, 3)"
  }
]