{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from fastai.vision.all import *"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-21T06:27:53.116941Z","iopub.status.busy":"2023-08-21T06:27:53.116584Z","iopub.status.idle":"2023-08-21T06:27:57.653663Z","shell.execute_reply":"2023-08-21T06:27:57.652087Z","shell.execute_reply.started":"2023-08-21T06:27:53.116910Z"},"trusted":true},"outputs":[],"source":["import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n","from pathlib import Path\n","from torch import tensor\n","from fastcore.test import test_close\n","torch.manual_seed(42)\n","\n","mpl.rcParams['image.cmap'] = 'gray'\n","torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n","np.set_printoptions(precision=2, linewidth=125)\n","\n","path_data = Path('../data')\n","path_gz = path_data/'mnist.pkl.gz'\n","with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n","x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Foundations version\n","## Basic architecture"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["(50000, 784, tensor(10))"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["n,m = x_train.shape\n","c = y_train.max()+1\n","n,m,c"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# num hidden\n","nh = 50"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["w1 = torch.randn(m, nh)\n","b1 = torch.zeros(nh)\n","w2 = torch.randn(nh, 1)\n","b2 = torch.zeros(1)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def lin(x, w, b): return x@w+b"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([10000, 50])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["t = lin(x_valid, w1, b1)\n","t.shape"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def relu(x): return x.clamp_min(0.)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[ 0.00, 11.87,  0.00,  ...,  5.48,  2.14, 15.30],\n","        [ 5.38, 10.21,  0.00,  ...,  0.88,  0.08, 20.23],\n","        [ 3.31,  0.12,  3.10,  ..., 16.89,  0.00, 24.74],\n","        ...,\n","        [ 4.01, 10.35,  0.00,  ...,  0.23,  0.00, 18.28],\n","        [10.62,  0.00, 10.72,  ...,  0.00,  0.00, 18.23],\n","        [ 2.84,  0.00,  1.43,  ...,  0.00,  5.75,  2.12]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["t = relu(t)\n","t"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def model(xb):\n","    l1 = lin(xb, w1, b1)\n","    l2 = relu(l1)\n","    return lin(l2, w2, b2)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([10000, 1])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["res = model(x_valid)\n","res.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Loss function: MSE"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([10000, 1]), torch.Size([10000]))"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["res.shape, y_valid.shape"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([10000])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["res.squeeze().shape"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([50000, 1])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["y_train, y_valid = y_train.float(), y_valid.float()\n","\n","preds = model(x_train)\n","preds.shape"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(4154.01)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["(y_valid - res.squeeze()).pow(2).mean()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def mse(output, targ): return (output[:,0]-targ).pow(2).mean()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["tensor(4308.76)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["mse(preds,y_train)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Gradients and backward pass"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/latex":["$\\displaystyle 2 x$"],"text/plain":["2*x"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["from sympy import symbols, diff\n","x, y = symbols('x y')\n","diff(x**2, x)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/latex":["$\\displaystyle 3 x^{2}$"],"text/plain":["3*x**2"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["diff(x**3, x)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/latex":["$\\displaystyle 6 x$"],"text/plain":["6*x"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["diff(3*x**2+8, x)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def lin_grad(inp, out, w, b):\n","    inp.g = out.g @ w.t() \n","    # w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n","    w.g = inp.T@out.g\n","    b.g = out.g.sum(0)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def forward_and_backward(inp, targ):\n","    # forward pass:\n","    l1 = lin(inp, w1, b1)\n","    l2 = relu(l1)\n","    out = lin(l2, w2, b2)\n","    diff = out[:,0] -targ\n","    loss = diff.pow(2).mean()\n","\n","    # backward pass:\n","    out.g = 2.*diff[:, None] / inp.shape[0]\n","    lin_grad(l2, out, w2, b2)\n","    l1.g = (l1>0).float() * l2.g\n","    lin_grad(inp, l1, w1, b1)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["forward_and_backward(x_train, y_train)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[    -0.00,     -0.01,      0.00,  ...,     -0.00,      0.00,      0.00],\n","        [    -0.03,     -0.03,      0.01,  ...,     -0.04,     -0.01,     -0.01],\n","        [     0.00,      0.00,     -0.00,  ...,      0.00,     -0.00,      0.00],\n","        ...,\n","        [    -0.00,     -0.02,      0.01,  ...,     -0.00,     -0.00,      0.00],\n","        [    -0.02,     -0.01,      0.01,  ...,     -0.01,      0.01,     -0.00],\n","        [    -0.00,     -0.00,      0.00,  ...,     -0.00,     -0.00,     -0.00]])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["x_train.g"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def get_grad(x): return x.g.clone()\n","chks = w1, w2, b1,b2,x_train\n","grads = w1g, w2g, b1g, b2g, ig = tuple(map(get_grad, chks))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def mkgrad(x): return x.clone().requires_grad_(True)\n","ptgrads = w12, w22, b12, b22, xt2 = tuple(map(mkgrad, chks))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def forward(inp, targ):\n","    l1 = lin(inp, w12, b12)\n","    l2 = relu(l1)\n","    out = lin(l2, w22, b22)\n","    return mse(out, targ)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["loss = forward(xt2, y_train)\n","loss.backward()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["for a, b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Refactor model\n","## Layers as classes"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Dunder call,是指可以直接调用python对象的一种语法糖"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["from typing import Any\n","\n","\n","class A:\n","    def __call__(self, *args: Any, **kwds: Any) -> Any:\n","        print(f'hi, {args}')"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["hi, ('python',)\n"]}],"source":["a = A()\n","a('python')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Python方法中，args和kwds都是函数的参数，但它们在使用上有一些区别。\n","\n","args是一个用于接收位置参数的元组，可以接受任意数量的位置参数。当调用函数时，传递的位置参数会被收集到args元组中。在函数内部，你可以使用args来访问和处理传递的位置参数。例如，如果使用func(1, 2, 3)来调用函数，那么args将是(1, 2, 3)。\n","\n","kwds是一个用于接收关键字参数的字典，可以接受任意数量的关键字参数。当调用函数时，传递的关键字参数会被收集到kwds字典中。在函数内部，你可以使用kwds来访问和处理传递的关键字参数。例如，如果使用func(a=1, b=2, c=3)来调用函数，那么kwds将是{'a': 1, 'b': 2, 'c': 3}。\n","\n","需要注意的是，在函数定义的参数列表中，*args表示接受任意数量的位置参数，**kwds表示接受任意数量的关键字参数。而在函数体内部，args和kwds是实际接收到的位置参数和关键字参数的引用。"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["from typing import Any\n","\n","class Relu:\n","\n","    def __call__(self, inp) -> Any:\n","        self.inp = inp\n","        self.out = inp.clamp_min(0.)\n","        return self.out\n","\n","    def backward(self):\n","        self.inp.g = (self.inp>0).float() * self.out.g"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["from typing import Any\n","\n","class Lin:\n","    def __init__(self, w, b) -> None:\n","        self.w = w\n","        self.b = b\n","\n","    def __call__(self, inp: Any, **kwds: Any) -> Any:\n","        self.inp = inp\n","        self.out = lin(inp, self.w, self.b)\n","        return self.out\n","    \n","    def backward(self):\n","        self.inp.g = self.out.g @ self.w.t()\n","        self.w.g = self.inp.t() @ self.out.g\n","        self.b.g = self.out.g.sum(0)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["from typing import Any\n","\n","\n","class Mse:\n","    def __call__(self, inp, targ) -> Any:\n","        self.inp, self.targ = inp, targ\n","        self.out = mse(inp, targ)\n","        return self.out\n","    \n","    def backward(self):\n","        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["from typing import Any\n","\n","\n","class Model:\n","    def __init__(self, w1, b1, w2, b2) -> None:\n","        self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)]\n","        self.loss = Mse()\n","\n","    def __call__(self, x, targ) -> Any:\n","        for l in self.layers: x = l(x)\n","        return self.loss(x, targ)\n","    \n","    def backward(self):\n","        self.loss.backward()\n","        for l in reversed(self.layers): l.backward()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["model = Model(w1, b1, w2, b2)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["loss = model(x_train, y_train)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["model.backward()"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["test_close(w2g, w2.g, eps=0.01)\n","test_close(b2g, b2.g, eps=0.01)\n","test_close(w1g, w1.g, eps=0.01)\n","test_close(b1g, b1.g, eps=0.01)\n","test_close(ig, x_train.g, eps=0.01)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Module.forward()"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["class Module():\n","    def __call__(self, *args):\n","        self.args = args\n","        self.out = self.forward(*args)\n","        return self.out\n","\n","    def forward(self): raise Exception('not implemented')\n","    def backward(self): self.bwd(self.out, *self.args)\n","    def bwd(self): raise Exception('not implemented')"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["class Relu(Module):\n","    def forward(self, inp): return inp.clamp_min(0.)\n","    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["class Lin(Module):\n","    def __init__(self, w, b): self.w,self.b = w,b\n","    def forward(self, inp): return inp@self.w + self.b\n","    def bwd(self, out, inp):\n","        inp.g = self.out.g @ self.w.t()\n","        self.w.g = inp.t() @ self.out.g\n","        self.b.g = self.out.g.sum(0)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["class Mse(Module):\n","    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n","    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["class Mse(Module):\n","    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n","    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["model = Model(w1, b1, w2, b2)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["loss = model(x_train, y_train)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["model.backward()"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["test_close(w2g, w2.g, eps=0.01)\n","test_close(b2g, b2.g, eps=0.01)\n","test_close(w1g, w1.g, eps=0.01)\n","test_close(b1g, b1.g, eps=0.01)\n","test_close(ig, x_train.g, eps=0.01)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Autograd"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["class Linear(nn.Module):\n","    def __init__(self, n_in, n_out) -> None:\n","        super().__init__()\n","        self.w = torch.randn(n_in, n_out).requires_grad_()\n","        self.b = torch.zeros(n_out).requires_grad_()\n","\n","    def forward(self, inp): return inp@self.w + self.b"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["from typing import Any\n","\n","\n","class Model(nn.Module):\n","    def __init__(self, n_in, nh, n_out):\n","        super().__init__()\n","        self.layers = [Linear(n_in, nh), nn.ReLU(), Linear(nh, n_out)]\n","\n","    def __call__(self, x, targ) -> Any:\n","        for l in self.layers: x = l(x)\n","        return F.mse_loss(x, targ[:, None])\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["model = Model(m, nh, 1)\n","loss = model(x_train, y_train)\n","loss.backward()"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([-19.60,  -2.40,  -0.12,   1.99,  12.78, -15.32, -18.45,   0.35,   3.75,  14.67,  10.81,  12.20,  -2.95, -28.33,\n","          0.76,  69.15, -21.86,  49.78,  -7.08,   1.45,  25.20,  11.27, -18.15, -13.13, -17.69, -10.42,  -0.13, -18.89,\n","        -34.81,  -0.84,  40.89,   4.45,  62.35,  31.70,  55.15,  45.13,   3.25,  12.75,  12.45,  -1.41,   4.55,  -6.02,\n","        -62.51,  -1.89,  -1.41,   7.00,   0.49,  18.72,  -4.84,  -6.52])"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["l0 = model.layers[0]\n","l0.b.grad"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
